"""
åŸºäºStreamDiffusionæ€æƒ³çš„PeRFlowæµæ°´çº¿æ‰¹é‡å»å™ª

å…³é”®æ´å¯Ÿï¼š
- ä½¿ç”¨latent bufferç»´æŠ¤å¤šä¸ªå›¾åƒçš„ä¸­é—´çŠ¶æ€
- æµæ°´çº¿å¤„ç†ï¼šä¸åŒå›¾åƒåŒæ—¶å¤„äºä¸åŒçš„å»å™ªé˜¶æ®µ
- ä¸€æ¬¡UNetè°ƒç”¨å¤„ç†æ‰€æœ‰é˜¶æ®µï¼Œä½†æ¯ä¸ªé˜¶æ®µå¯¹åº”ä¸åŒçš„å›¾åƒ
"""

import time
from typing import List, Optional, Union, Any, Dict, Tuple, Literal

import numpy as np
import PIL.Image
import torch
from diffusers import StableDiffusionPipeline
from diffusers.image_processor import VaeImageProcessor
from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img import (
    retrieve_latents,
)

from .image_utils import postprocess_image


class PipelineBatchStreamFlow:
    """
    åŸºäºæµæ°´çº¿çš„PeRFlowæ‰¹é‡å»å™ª
    
    åŸç†ï¼š
    - ç»´æŠ¤4ä¸ªlatent bufferï¼Œåˆ†åˆ«å¯¹åº”4ä¸ªå»å™ªé˜¶æ®µ
    - æ¯æ¬¡è°ƒç”¨å¤„ç†4å¼ ä¸åŒå›¾åƒçš„ä¸åŒé˜¶æ®µ
    - å®ç°çœŸæ­£çš„æµæ°´çº¿å¹¶è¡Œï¼Œé¿å…ç ´ååºåˆ—ä¾èµ–
    """
    
    def __init__(
        self,
        pipe: StableDiffusionPipeline,
        t_index_list: List[int],
        torch_dtype: torch.dtype = torch.float16,
        width: int = 512,
        height: int = 512,
        do_add_noise: bool = True,
        frame_buffer_size: int = 1,
        cfg_type: Literal["none", "full", "self", "initialize"] = "full",
        use_pipeline_batch: bool = True,
        vae_decode_method: str = "normalize",
    ) -> None:
        self.device = pipe.device
        self.dtype = torch_dtype
        self.generator = None

        self.height = height
        self.width = width

        self.latent_height = int(height // pipe.vae_scale_factor)
        self.latent_width = int(width // pipe.vae_scale_factor)

        self.frame_bff_size = frame_buffer_size
        self.denoising_steps_num = len(t_index_list)
        self.cfg_type = cfg_type
        self.vae_decode_method = vae_decode_method
        self.use_pipeline_batch = use_pipeline_batch

        self.t_list = t_index_list
        self.do_add_noise = do_add_noise

        self.pipe = pipe
        self.image_processor = VaeImageProcessor(pipe.vae_scale_factor)

        # ä¿æŒåŸå§‹è°ƒåº¦å™¨
        self.scheduler = pipe.scheduler
        self.text_encoder = pipe.text_encoder
        self.unet = pipe.unet
        self.vae = pipe.vae

        self.inference_time_ema = 0

        # ç¼“å­˜å˜é‡
        self.prompt_embeds = None
        self.negative_prompt_embeds = None
        self.guidance_scale = 7.5

        # æµæ°´çº¿çŠ¶æ€
        self.pipeline_initialized = False
        self.latent_buffer = None  # å­˜å‚¨ä¸åŒé˜¶æ®µçš„latent
        self.step_counter = 0

    def prepare(
        self,
        prompt: str,
        negative_prompt: str = "",
        num_inference_steps: int = 4,
        guidance_scale: float = 7.5,
        delta: float = 1.0,
    ) -> None:
        """å‡†å¤‡å‡½æ•°"""
        self.guidance_scale = guidance_scale
        
        # ç¼–ç æç¤ºè¯
        do_classifier_free_guidance = guidance_scale > 1.0
        prompt_embeds = self.pipe.encode_prompt(
            prompt=prompt,
            device=self.device,
            num_images_per_prompt=1,
            do_classifier_free_guidance=do_classifier_free_guidance,
            negative_prompt=negative_prompt,
        )
        
        self.prompt_embeds = prompt_embeds[0]
        self.negative_prompt_embeds = prompt_embeds[1] if do_classifier_free_guidance else None

        # ä½¿ç”¨PeRFlowçš„åŸç”Ÿæ—¶é—´æ­¥è®¾ç½®
        self.scheduler.set_timesteps(num_inference_steps, self.device)
        self.timesteps = self.scheduler.timesteps.to(self.device)
        
        # æ ¹æ®t_index_listé€‰æ‹©å­æ—¶é—´æ­¥
        self.sub_timesteps = []
        for t_idx in self.t_list:
            if t_idx < len(self.timesteps):
                self.sub_timesteps.append(self.timesteps[t_idx])
            else:
                self.sub_timesteps.append(self.timesteps[-1])
        
        self.sub_timesteps_tensor = torch.stack(self.sub_timesteps)
        
        # åˆå§‹åŒ–æµæ°´çº¿buffer
        if self.use_pipeline_batch and not self.pipeline_initialized:
            self._initialize_pipeline()
        
        print(f"PeRFlowæ—¶é—´æ­¥: {self.timesteps.tolist()}")
        print(f"é€‰æ‹©çš„å­æ—¶é—´æ­¥: {[t.item() for t in self.sub_timesteps]}")
        print(f"æµæ°´çº¿æ‰¹å¤„ç†: {'âœ… å¯ç”¨' if self.use_pipeline_batch else 'âŒ ç¦ç”¨'}")

    def _initialize_pipeline(self):
        """åˆå§‹åŒ–æµæ°´çº¿buffer"""
        # åˆ›å»ºéšæœºçš„åˆå§‹latent buffer
        # è¿™äº›bufferä»£è¡¨ä¸åŒå›¾åƒçš„ä¸åŒé˜¶æ®µ
        self.latent_buffer = torch.randn(
            (self.denoising_steps_num - 1, 4, self.latent_height, self.latent_width),
            device=self.device,
            dtype=self.dtype
        )
        self.pipeline_initialized = True
        print(f"âœ… æµæ°´çº¿bufferåˆå§‹åŒ–: {self.latent_buffer.shape}")

    def predict_x0_pipeline_batch(self, x_t_latent: torch.Tensor) -> torch.Tensor:
        """
        æµæ°´çº¿æ‰¹é‡å»å™ª
        
        å…³é”®æ€æƒ³ï¼š
        - x_t_latent: æ–°å›¾åƒçš„ç¬¬1é˜¶æ®µ
        - latent_buffer[0]: æŸå›¾åƒçš„ç¬¬2é˜¶æ®µ
        - latent_buffer[1]: æŸå›¾åƒçš„ç¬¬3é˜¶æ®µ  
        - latent_buffer[2]: æŸå›¾åƒçš„ç¬¬4é˜¶æ®µ
        
        ä¸€æ¬¡UNetè°ƒç”¨å¤„ç†æ‰€æœ‰è¿™äº›ä¸åŒé˜¶æ®µ
        """
        if not self.use_pipeline_batch or not self.pipeline_initialized:
            return self.predict_x0_perflow_original(x_t_latent)
        
        # æ„å»ºæµæ°´çº¿æ‰¹é‡è¾“å…¥
        if self.denoising_steps_num > 1:
            # å°†æ–°è¾“å…¥ä¸bufferä¸­çš„ä¸­é—´çŠ¶æ€ç»„åˆ
            pipeline_latents = torch.cat([x_t_latent, self.latent_buffer], dim=0)
        else:
            pipeline_latents = x_t_latent
        
        # æ—¶é—´æ­¥å¯¹åº”ä¸åŒçš„é˜¶æ®µ
        timestep_batch = self.sub_timesteps_tensor
        
        # CFGå¤„ç†
        use_cfg = self.guidance_scale > 1.0 and self.cfg_type != "none"
        
        if use_cfg:
            latent_model_input = torch.cat([pipeline_latents, pipeline_latents], dim=0)
            timestep_input = torch.cat([timestep_batch, timestep_batch], dim=0)
            
            if self.negative_prompt_embeds is not None:
                batch_prompt_embeds = torch.cat([
                    self.negative_prompt_embeds.repeat(self.denoising_steps_num, 1, 1),
                    self.prompt_embeds.repeat(self.denoising_steps_num, 1, 1)
                ], dim=0)
            else:
                batch_prompt_embeds = self.prompt_embeds.repeat(self.denoising_steps_num * 2, 1, 1)
        else:
            latent_model_input = pipeline_latents
            timestep_input = timestep_batch
            batch_prompt_embeds = self.prompt_embeds.repeat(self.denoising_steps_num, 1, 1)
        
        # ğŸš€ æµæ°´çº¿UNetè°ƒç”¨ï¼šä¸€æ¬¡å¤„ç†æ‰€æœ‰é˜¶æ®µ
        with torch.no_grad():
            noise_pred_batch = self.unet(
                latent_model_input,
                timestep_input,
                encoder_hidden_states=batch_prompt_embeds,
                return_dict=False,
            )[0]
        
        # CFGåå¤„ç†
        if use_cfg:
            noise_pred_uncond, noise_pred_text = noise_pred_batch.chunk(2)
            noise_pred_batch = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)
        
        # åº”ç”¨scheduleræ­¥éª¤åˆ°æ¯ä¸ªé˜¶æ®µ
        processed_latents = []
        
        for i in range(self.denoising_steps_num):
            latent = pipeline_latents[i].unsqueeze(0)
            noise_pred = noise_pred_batch[i].unsqueeze(0)
            t = self.sub_timesteps[i]
            
            # åº”ç”¨PeRFlow scheduler
            step_result = self.scheduler.step(noise_pred, t, latent, return_dict=False)
            processed_latents.append(step_result[0])
        
        # æ›´æ–°bufferï¼šå‘å‰ç§»åŠ¨æµæ°´çº¿
        if self.denoising_steps_num > 1:
            # è¾“å‡ºæ˜¯æœ€åä¸€ä¸ªé˜¶æ®µçš„ç»“æœ
            output = processed_latents[-1]
            
            # æ›´æ–°bufferï¼šå‰ç§»æµæ°´çº¿
            # æ–°çš„buffer[0] = å½“å‰è¾“å…¥çš„ç¬¬1æ­¥ç»“æœï¼ˆå°†è¿›å…¥ç¬¬2é˜¶æ®µï¼‰
            # æ–°çš„buffer[1] = ä¹‹å‰buffer[0]çš„ç»“æœï¼ˆå°†è¿›å…¥ç¬¬3é˜¶æ®µï¼‰
            # æ–°çš„buffer[2] = ä¹‹å‰buffer[1]çš„ç»“æœï¼ˆå°†è¿›å…¥ç¬¬4é˜¶æ®µï¼‰
            new_buffer = []
            
            # å½“å‰è¾“å…¥å¤„ç†åè¿›å…¥ä¸‹ä¸€é˜¶æ®µ
            new_buffer.append(processed_latents[0])
            
            # ä¹‹å‰bufferä¸­çš„çŠ¶æ€ç»§ç»­å‰è¿›
            for i in range(self.denoising_steps_num - 2):
                new_buffer.append(processed_latents[i + 1])
            
            self.latent_buffer = torch.cat(new_buffer, dim=0)
            
            return output
        else:
            return processed_latents[0]

    def predict_x0_perflow_original(self, x_t_latent: torch.Tensor) -> torch.Tensor:
        """åŸå§‹é€æ­¥å»å™ªæ–¹æ³•"""
        latents = x_t_latent
        use_cfg = self.guidance_scale > 1.0 and self.cfg_type != "none"

        for i, t in enumerate(self.sub_timesteps):
            if use_cfg:
                latent_model_input = torch.cat([latents] * 2)
                if self.negative_prompt_embeds is not None:
                    prompt_embeds = torch.cat([self.negative_prompt_embeds, self.prompt_embeds])
                else:
                    prompt_embeds = torch.cat([self.prompt_embeds, self.prompt_embeds])
            else:
                latent_model_input = latents
                prompt_embeds = self.prompt_embeds

            # ğŸ”§ TensorRTå…¼å®¹ï¼šç¡®ä¿timestepæ˜¯æ­£ç¡®shapeçš„tensor [batch_size]
            batch_size = latent_model_input.shape[0]
            if isinstance(t, torch.Tensor) and t.dim() == 0:
                # æ ‡é‡tensor -> [batch_size] tensor
                timestep = t.unsqueeze(0).repeat(batch_size)
            elif isinstance(t, torch.Tensor):
                timestep = t
            else:
                # å¦‚æœæ˜¯Pythonæ•°å€¼ï¼Œè½¬æ¢ä¸ºtensor
                timestep = torch.tensor([t] * batch_size, device=self.device, dtype=torch.long)

            with torch.no_grad():
                noise_pred = self.unet(
                    latent_model_input,
                    timestep,  # ğŸ”§ ä½¿ç”¨æ­£ç¡®shapeçš„timestep
                    encoder_hidden_states=prompt_embeds,
                    return_dict=False,
                )[0]

            if use_cfg:
                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)

            step_result = self.scheduler.step(noise_pred, t, latents, return_dict=False)
            latents = step_result[0]

        return latents

    def decode_image_perflow(self, x_0_pred_out: torch.Tensor) -> torch.Tensor:
        """VAEè§£ç """
        with torch.no_grad():
            # Check if this is WSG VAE and handle dtype properly
            if hasattr(self.vae, 'decode_rgb_only_by_default'):
                # WSG VAE might use different dtype (e.g., float32)
                # Convert input to VAE's dtype if needed
                if hasattr(self.vae, 'dtype'):
                    vae_dtype = self.vae.dtype
                else:
                    try:
                        vae_dtype = next(self.vae.parameters()).dtype
                    except StopIteration:
                        vae_dtype = self.dtype
                
                if x_0_pred_out.dtype != vae_dtype:
                    x_0_pred_out = x_0_pred_out.to(dtype=vae_dtype)
                
                output_latent = self.vae.decode(
                    x_0_pred_out / self.vae.config.scaling_factor, return_dict=False
                )[0]
                
                # WSG VAE outputs are already properly normalized
                # Keep the output in VAE's dtype
                return output_latent
            else:
                # Standard VAE decode
                output_latent = self.vae.decode(
                    x_0_pred_out / self.vae.config.scaling_factor, return_dict=False
                )[0]
                
                # Standard VAE needs postprocessing
                output_latent = postprocess_image(
                    output_latent, 
                    output_type="pt", 
                    denormalize_method=self.vae_decode_method
                )
                
                return output_latent

    @torch.no_grad()
    def txt2img(self, batch_size: int = 1) -> torch.Tensor:
        """æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ"""
        x_t_latent = torch.randn(
            (batch_size, 4, self.latent_height, self.latent_width),
            device=self.device,
            dtype=self.dtype
        )

        # ä½¿ç”¨æµæ°´çº¿æ‰¹é‡å»å™ª
        x_0_pred_out = self.predict_x0_pipeline_batch(x_t_latent)
        x_output = self.decode_image_perflow(x_0_pred_out).detach().clone()

        self.step_counter += 1

        return x_output

    @torch.no_grad()
    def generate_latent(self, batch_size: int = 1) -> torch.Tensor:
        """åªç”Ÿæˆlatentï¼Œä¸è§£ç ï¼ˆç”¨äºæ‰¹é‡VAEè§£ç ä¼˜åŒ–ï¼‰"""
        x_t_latent = torch.randn(
            (batch_size, 4, self.latent_height, self.latent_width),
            device=self.device,
            dtype=self.dtype
        )

        # ä½¿ç”¨æµæ°´çº¿æ‰¹é‡å»å™ª
        x_0_pred_out = self.predict_x0_pipeline_batch(x_t_latent)

        self.step_counter += 1

        return x_0_pred_out

    @torch.no_grad()
    def decode_latents(self, latents: torch.Tensor) -> torch.Tensor:
        """æ‰¹é‡è§£ç latentsï¼ˆæ”¯æŒbatchï¼‰"""
        return self.decode_image_perflow(latents).detach().clone()

    def get_inference_time(self) -> float:
        """è·å–å¹³å‡æ¨ç†æ—¶é—´"""
        return self.inference_time_ema
    
    def reset_pipeline(self):
        """é‡ç½®æµæ°´çº¿çŠ¶æ€"""
        self.pipeline_initialized = False
        self.latent_buffer = None
        self.step_counter = 0