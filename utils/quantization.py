"""
GPUå…¼å®¹çš„VAE INT8é‡åŒ–å·¥å…·
"""

import torch
import torch.nn as nn
import os
from diffusers import AutoencoderTiny


class QuantizedLayer(nn.Module):
    """
    é‡åŒ–å±‚ï¼šå­˜å‚¨INT8æƒé‡ï¼Œæ¨ç†æ—¶åŠ¨æ€åé‡åŒ–
    """
    def __init__(self, original_layer, weight_int8, scale):
        super().__init__()
        self.layer_type = type(original_layer)

        # å­˜å‚¨INT8æƒé‡å’Œscale
        self.register_buffer('weight_int8', weight_int8)
        self.register_buffer('scale', torch.tensor(scale, dtype=torch.float32))

        # ä¿å­˜bias
        if original_layer.bias is not None:
            self.register_buffer('bias', original_layer.bias.data)
        else:
            self.bias = None

        # ä¿å­˜å±‚å‚æ•°
        if isinstance(original_layer, (nn.Conv2d, nn.ConvTranspose2d)):
            self.stride = original_layer.stride
            self.padding = original_layer.padding
            self.dilation = original_layer.dilation
            self.groups = original_layer.groups
            if isinstance(original_layer, nn.ConvTranspose2d):
                self.output_padding = original_layer.output_padding

        # ç¼“å­˜åé‡åŒ–åçš„æƒé‡ï¼ˆé¿å…é‡å¤è½¬æ¢ï¼‰
        self._cached_weight = None
        self._cached_dtype = None

    def forward(self, x):
        """å‰å‘ï¼šä½¿ç”¨ç¼“å­˜çš„FP16æƒé‡"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åé‡åŒ–
        if self._cached_weight is None or self._cached_dtype != x.dtype:
            # ç¬¬ä¸€æ¬¡æˆ–dtypeæ”¹å˜æ—¶åé‡åŒ–
            self._cached_weight = self.weight_int8.to(x.dtype) * self.scale
            self._cached_dtype = x.dtype

        weight = self._cached_weight

        # æ ¹æ®å±‚ç±»å‹è®¡ç®—
        if self.layer_type == nn.Conv2d:
            return nn.functional.conv2d(
                x, weight, self.bias,
                self.stride, self.padding, self.dilation, self.groups
            )
        elif self.layer_type == nn.ConvTranspose2d:
            return nn.functional.conv_transpose2d(
                x, weight, self.bias,
                self.stride, self.padding, self.output_padding,
                self.groups, self.dilation
            )
        elif self.layer_type == nn.Linear:
            return nn.functional.linear(x, weight, self.bias)
        else:
            raise NotImplementedError(f"Unsupported layer: {self.layer_type}")


class Int8QuantizedVAE(nn.Module):
    """
    INT8é‡åŒ–VAEï¼ˆGPUå…¼å®¹ï¼‰
    decoderæƒé‡é‡åŒ–ä¸ºINT8ï¼Œæ¨ç†æ—¶åŠ¨æ€åé‡åŒ–
    """
    def __init__(self, vae):
        super().__init__()
        self.config = vae.config

        # å¤åˆ¶å„ä¸ªç»„ä»¶
        self.decoder = vae.decoder
        if hasattr(vae, 'encoder'):
            self.encoder = vae.encoder
        if hasattr(vae, 'quant_conv'):
            self.quant_conv = vae.quant_conv
        if hasattr(vae, 'post_quant_conv'):
            self.post_quant_conv = vae.post_quant_conv

    def decode(self, latent, **kwargs):
        """è§£ç latentä¸ºå›¾åƒ"""
        if hasattr(self, 'post_quant_conv'):
            latent = self.post_quant_conv(latent)
        image = self.decoder(latent)
        return (image,)

    def encode(self, image, **kwargs):
        """ç¼–ç å›¾åƒä¸ºlatent"""
        if hasattr(self, 'encoder'):
            latent = self.encoder(image)
            if hasattr(self, 'quant_conv'):
                latent = self.quant_conv(latent)
            return latent
        raise NotImplementedError("Encoder not available")

    def forward(self, *args, **kwargs):
        return self.decode(*args, **kwargs)


def quantize_vae_decoder(vae):
    """
    é‡åŒ–VAEçš„decoderä¸ºINT8

    å‚æ•°:
        vae: AutoencoderTinyæˆ–AutoencoderKL

    è¿”å›:
        Int8QuantizedVAE
    """
    quantized_vae = Int8QuantizedVAE(vae)

    # é‡åŒ–decoder
    quantized_vae.decoder = _quantize_module(vae.decoder)

    return quantized_vae


def _quantize_module(module):
    """é€’å½’é‡åŒ–æ¨¡å—ä¸­çš„Convå’ŒLinearå±‚"""
    for name, child in module.named_children():
        if isinstance(child, (nn.Conv2d, nn.Linear, nn.ConvTranspose2d)):
            # é‡åŒ–è¿™ä¸€å±‚
            setattr(module, name, _quantize_layer(child))
        else:
            # é€’å½’å¤„ç†å­æ¨¡å—
            _quantize_module(child)
    return module


def _quantize_layer(layer):
    """é‡åŒ–å•ä¸ªå±‚çš„æƒé‡ä¸ºINT8"""
    weight = layer.weight.data

    # è®¡ç®—é‡åŒ–scaleï¼ˆper-tensorï¼‰
    weight_max = weight.abs().max()
    scale = weight_max / 127.0  # INT8èŒƒå›´ [-127, 127]

    # é‡åŒ–æƒé‡
    weight_int8 = torch.clamp(
        torch.round(weight / scale), -127, 127
    ).to(torch.int8)

    return QuantizedLayer(layer, weight_int8, scale)


def load_quantized_tinyvae(device, dtype):
    """
    åŠ è½½é¢„é‡åŒ–çš„TinyVAE

    å‚æ•°:
        device: ç›®æ ‡è®¾å¤‡ (cuda/cpu)
        dtype: æ•°æ®ç±»å‹ (torch.float16ç­‰)

    è¿”å›:
        é‡åŒ–åçš„VAEæ¨¡å‹
    """
    model_path = "models/tinyvae_int8.pth"

    if not os.path.exists(model_path):
        print(f"âŒ é‡åŒ–æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
        print(f"   è¯·å…ˆè¿è¡Œ: python -m utils.quantization")
        print(f"   å›é€€åˆ°åœ¨çº¿é‡åŒ–...")

        # åœ¨çº¿é‡åŒ–
        print("ğŸ”¬ åœ¨çº¿é‡åŒ–TinyVAE...")
        vae = AutoencoderTiny.from_pretrained("madebyollin/taesd")
        quantized_vae = quantize_vae_decoder(vae)
        quantized_vae = quantized_vae.to(device=device, dtype=dtype)
        print("   âœ… åœ¨çº¿é‡åŒ–å®Œæˆ")
        return quantized_vae

    print(f"ğŸ“¦ åŠ è½½é¢„é‡åŒ–TinyVAE: {model_path}")

    # åŠ è½½checkpoint
    checkpoint = torch.load(model_path, map_location='cpu')

    # åˆ›å»ºVAEå¹¶é‡åŒ–ç»“æ„ï¼ˆè¿™æ ·ç»“æ„å°±å’Œä¿å­˜çš„åŒ¹é…ï¼‰
    temp_vae = AutoencoderTiny.from_pretrained("madebyollin/taesd")
    quantized_vae = quantize_vae_decoder(temp_vae)  # é‡åŒ–ç»“æ„

    # åŠ è½½é‡åŒ–æƒé‡
    quantized_vae.load_state_dict(checkpoint['model_state_dict'])

    # ç§»åˆ°ç›®æ ‡è®¾å¤‡
    quantized_vae = quantized_vae.to(device=device, dtype=dtype)

    print(f"   âœ… INT8é‡åŒ–VAEåŠ è½½æˆåŠŸ")
    print(f"   ç²¾åº¦: INT8æƒé‡ + {dtype}æ¨ç†")

    return quantized_vae


def quantize_and_save_tinyvae():
    """
    ç¦»çº¿é‡åŒ–TinyVAEå¹¶ä¿å­˜
    è¿è¡Œ: python -m utils.quantization
    """
    print("=" * 60)
    print("TinyVAE INT8 é‡åŒ–å·¥å…·")
    print("=" * 60)

    # åŠ è½½åŸå§‹TinyVAE
    print("\nğŸ“¦ åŠ è½½TinyVAE...")
    vae = AutoencoderTiny.from_pretrained("madebyollin/taesd")
    vae.eval()

    print(f"   æ¨¡å‹: {type(vae)}")
    print(f"   å‚æ•°é‡: {sum(p.numel() for p in vae.parameters()):,}")

    # é‡åŒ–
    print("\nğŸ”¬ å¼€å§‹é‡åŒ–...")
    quantized_vae = quantize_vae_decoder(vae)

    # æµ‹è¯•é‡åŒ–æ¨¡å‹
    print("\nğŸ§ª æµ‹è¯•é‡åŒ–æ¨¡å‹...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    quantized_vae = quantized_vae.to(device=device, dtype=torch.float16)

    test_latent = torch.randn(1, 4, 64, 64, device=device, dtype=torch.float16)

    with torch.no_grad():
        output = quantized_vae.decode(test_latent / quantized_vae.config.scaling_factor)

    image = output[0] if isinstance(output, tuple) else output
    print(f"   âœ… æµ‹è¯•é€šè¿‡")
    print(f"   è¾“å…¥: {test_latent.shape}")
    print(f"   è¾“å‡º: {image.shape}")
    print(f"   èŒƒå›´: [{image.min():.3f}, {image.max():.3f}]")

    # å¯¹æ¯”è´¨é‡
    print("\nğŸ“Š è´¨é‡å¯¹æ¯”...")
    vae_original = vae.to(device=device, dtype=torch.float16)
    with torch.no_grad():
        output_original = vae_original.decode(test_latent / vae.config.scaling_factor)

    # æå–å›¾åƒtensor
    if hasattr(output_original, 'sample'):
        image_original = output_original.sample
    elif isinstance(output_original, tuple):
        image_original = output_original[0]
    else:
        image_original = output_original

    mse = ((image - image_original) ** 2).mean().item()
    psnr = 20 * torch.log10(torch.tensor(1.0)) - 10 * torch.log10(torch.tensor(mse))
    print(f"   MSE: {mse:.6f}")
    print(f"   PSNR: {psnr:.2f} dB")
    print(f"   {'âœ… è´¨é‡æŸå¤±å¯æ¥å—' if mse < 0.001 else 'âš ï¸  è´¨é‡æŸå¤±è¾ƒå¤§'}")

    # æ˜¾å­˜å¯¹æ¯”
    print("\nğŸ’¾ æ˜¾å­˜å¯¹æ¯”...")
    original_size = sum(p.numel() * p.element_size() for p in vae.parameters())
    quantized_size = sum(p.numel() * p.element_size() for p in quantized_vae.parameters())

    print(f"   åŸå§‹: {original_size / 1024 / 1024:.2f} MB")
    print(f"   é‡åŒ–: {quantized_size / 1024 / 1024:.2f} MB")
    print(f"   èŠ‚çœ: {(1 - quantized_size / original_size) * 100:.1f}%")

    # ä¿å­˜
    os.makedirs("models", exist_ok=True)
    output_path = "models/tinyvae_int8.pth"
    print(f"\nğŸ’¾ ä¿å­˜é‡åŒ–æ¨¡å‹åˆ°: {output_path}")

    # ç§»å›CPUä¿å­˜
    quantized_vae_cpu = quantized_vae.cpu()

    torch.save({
        'model_state_dict': quantized_vae_cpu.state_dict(),
        'config': vae.config,
        'quantization_info': {
            'method': 'per-tensor INT8',
            'target': 'decoder weights only',
            'dtype': 'int8',
            'mse': mse,
            'psnr': psnr.item(),
        }
    }, output_path)

    print(f"   âœ… ä¿å­˜æˆåŠŸ")
    print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB")

    print("\n" + "=" * 60)
    print("âœ… é‡åŒ–å®Œæˆï¼")
    print("=" * 60)
    print(f"\nä½¿ç”¨æ–¹æ³•:")
    print(f"1. åœ¨test_demo_gen.pyä¸­è®¾ç½®: USE_INT8_VAE = True")
    print(f"2. è¿è¡Œæµ‹è¯•ï¼Œè‡ªåŠ¨åŠ è½½ {output_path}")
    print()


# å…è®¸ä½œä¸ºè„šæœ¬è¿è¡Œ
if __name__ == "__main__":
    quantize_and_save_tinyvae()
