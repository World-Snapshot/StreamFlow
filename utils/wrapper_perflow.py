import gc
import os
from pathlib import Path
import traceback
from typing import List, Literal, Optional, Union, Dict

import numpy as np
import torch
from diffusers import AutoencoderTiny, StableDiffusionPipeline
from PIL import Image

from src.streamflow import StreamFlow
from src.streamflow.image_utils import postprocess_image
from src.scheduler_perflow import PeRFlowScheduler

torch.set_grad_enabled(False)
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True


class PeRFlowWrapper:
    def __init__(
        self,
        model_id_or_path: str = "hansyan/perflow-sd15-dreamshaper",
        t_index_list: List[int] = [0, 1, 2, 3],
        lora_dict: Optional[Dict[str, float]] = None,
        mode: Literal["img2img", "txt2img"] = "txt2img",
        output_type: Literal["pil", "pt", "np", "latent"] = "pil",
        vae_decode_method: Literal["normalize", "dynamic", "clamp"] = "normalize",
        device: Literal["cpu", "cuda"] = "cuda",
        dtype: torch.dtype = torch.float16,
        frame_buffer_size: int = 1,
        width: int = 512,
        height: int = 512,
        warmup: int = 5,
        acceleration: Literal["none", "xformers", "tensorrt"] = "xformers",
        do_add_noise: bool = True,
        use_tiny_vae: bool = False,
        cfg_type: Literal["none", "full", "self", "initialize"] = "full",
        seed: int = 2,
        num_inference_steps: int = 4,
        guidance_scale: float = 7.5,
    ):
        """
        PeRFlowä¸“ç”¨åŒ…è£…å™¨ï¼Œä¼˜åŒ–äº†è´¨é‡å’Œæ€§èƒ½
        
        Parameters
        ----------
        model_id_or_path : str
            PeRFlowæ¨¡å‹è·¯å¾„ï¼Œé»˜è®¤ "hansyan/perflow-sd15-dreamshaper"
        t_index_list : List[int]
            æ—¶é—´æ­¥ç´¢å¼•åˆ—è¡¨ï¼ŒPeRFlowæ¨è [0, 1, 2, 3]
        vae_decode_method : Literal["normalize", "dynamic", "clamp"]
            VAEè§£ç åå¤„ç†æ–¹æ³•ï¼š
            - "normalize": æ ‡å‡†å½’ä¸€åŒ– (image / 2 + 0.5) - æ¨è
            - "dynamic": åŠ¨æ€èŒƒå›´å½’ä¸€åŒ– - æœ€å¤§åŠ¨æ€èŒƒå›´ä½†å¯èƒ½æœ‰è‰²å
            - "clamp": ç›´æ¥æˆªæ–­ - ä¼šåæš—ï¼Œä¸æ¨è
        use_tiny_vae : bool
            æ˜¯å¦ä½¿ç”¨TinyVAEåŠ é€Ÿï¼ŒFalseä½¿ç”¨åŸå§‹VAEä¿è¯è´¨é‡
        num_inference_steps : int
            æ¨ç†æ­¥æ•°ï¼ŒPeRFlowæ¨è4æ­¥
        guidance_scale : float
            å¼•å¯¼ç¼©æ”¾ï¼ŒPeRFlowæ¨è7.5
        """
        
        self.device = device
        self.dtype = dtype
        self.mode = mode
        self.output_type = output_type
        self.vae_decode_method = vae_decode_method
        self.warmup = warmup
        self.num_inference_steps = num_inference_steps
        self.guidance_scale = guidance_scale
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(seed)
        np.random.seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
        
        print(f"ğŸš€ åˆå§‹åŒ–PeRFlowWrapper...")
        print(f"   æ¨¡å‹: {model_id_or_path}")
        print(f"   æ¨¡å¼: {mode}")
        print(f"   VAEè§£ç æ–¹æ³•: {vae_decode_method}")
        print(f"   æ—¶é—´æ­¥: {t_index_list}")
        print(f"   æ¨ç†æ­¥æ•°: {num_inference_steps}")
        
        # åŠ è½½ç®¡é“
        self.pipe = StableDiffusionPipeline.from_pretrained(
            model_id_or_path, 
            torch_dtype=dtype,
            use_safetensors=True,
        )
        
        # è®¾ç½®PeRFlowè°ƒåº¦å™¨
        self.pipe.scheduler = PeRFlowScheduler.from_config(
            self.pipe.scheduler.config, 
            prediction_type="diff_eps", 
            num_time_windows=4
        )
        
        self.pipe.to(device, dtype)
        
        # åº”ç”¨åŠ é€Ÿ
        if acceleration == "xformers":
            self.pipe.enable_xformers_memory_efficient_attention()
        elif acceleration == "tensorrt":
            # è¿™é‡Œå¯ä»¥æ·»åŠ TensorRTåŠ é€Ÿé€»è¾‘
            print("âš ï¸  TensorRTåŠ é€Ÿæš‚æœªå®ç°ï¼Œä½¿ç”¨xformers")
            self.pipe.enable_xformers_memory_efficient_attention()
        
        # å¯é€‰ä½¿ç”¨TinyVAE
        if use_tiny_vae:
            print("ğŸ”„ åŠ è½½TinyVAE...")
            self.pipe.vae = AutoencoderTiny.from_pretrained(
                "madebyollin/taesd"
            ).to(device, dtype)
            print("âœ… TinyVAEåŠ è½½å®Œæˆ")
        
        # åŠ è½½LoRA
        if lora_dict:
            for lora_name, lora_scale in lora_dict.items():
                self.pipe.load_lora_weights(lora_name, adapter_name=lora_name)
                print(f"âœ… åŠ è½½LoRA: {lora_name} (scale: {lora_scale})")
        
        # åˆ›å»ºStreamFlow
        self.stream = StreamFlow(
            self.pipe,
            t_index_list=t_index_list,
            torch_dtype=dtype,
            width=width,
            height=height,
            do_add_noise=do_add_noise,
            frame_buffer_size=frame_buffer_size,
            cfg_type=cfg_type,
            use_original_scheduler=True,
            vae_decode_method=vae_decode_method,
        )
        
        print("âœ… PeRFlowWrapperåˆå§‹åŒ–å®Œæˆ")
    
    def prepare(
        self,
        prompt: str,
        negative_prompt: str = "",
        num_inference_steps: Optional[int] = None,
        guidance_scale: Optional[float] = None,
    ):
        """
        å‡†å¤‡æ¨ç†
        """
        if num_inference_steps is None:
            num_inference_steps = self.num_inference_steps
        if guidance_scale is None:
            guidance_scale = self.guidance_scale
            
        self.stream.prepare(
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
        )
        
        # é¢„çƒ­
        print(f"ğŸ”¥ é¢„çƒ­ä¸­... ({self.warmup}æ¬¡)")
        if self.mode == "txt2img":
            for _ in range(self.warmup):
                self.stream.txt2img()
        else:
            # img2imgæ¨¡å¼çš„é¢„çƒ­éœ€è¦è¾“å…¥å›¾åƒ
            dummy_image = torch.randn(1, 3, 512, 512).to(self.device, self.dtype)
            for _ in range(self.warmup):
                self.stream(dummy_image)
        
        print("âœ… é¢„çƒ­å®Œæˆ")
    
    def __call__(
        self, 
        image: Optional[Union[str, Image.Image, torch.Tensor, np.ndarray]] = None
    ) -> Union[Image.Image, torch.Tensor, np.ndarray]:
        """
        æ‰§è¡Œæ¨ç†
        """
        if self.mode == "txt2img":
            result = self.stream.txt2img()
        else:
            if image is None:
                raise ValueError("img2imgæ¨¡å¼éœ€è¦è¾“å…¥å›¾åƒ")
            
            # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥å›¾åƒ
            if isinstance(image, str):
                image = Image.open(image)
            elif isinstance(image, np.ndarray):
                image = Image.fromarray(image)
            
            result = self.stream(image)
        
        # åå¤„ç†è¾“å‡ºæ ¼å¼
        if self.output_type == "pt":
            return result
        elif self.output_type == "np":
            return result.cpu().numpy()
        elif self.output_type == "pil":
            # è½¬æ¢ä¸ºPIL
            result_np = result.squeeze(0).permute(1, 2, 0).cpu().numpy()
            result_np = (result_np * 255).astype(np.uint8)
            return Image.fromarray(result_np)
        elif self.output_type == "latent":
            # è¿”å›æ½œåœ¨è¡¨ç¤ºéœ€è¦ä¿®æ”¹StreamFlow
            raise NotImplementedError("latentè¾“å‡ºç±»å‹æš‚æœªå®ç°")
    
    def txt2img(self) -> Union[Image.Image, torch.Tensor, np.ndarray]:
        """
        æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ
        """
        return self.__call__()
    
    def img2img(self, image) -> Union[Image.Image, torch.Tensor, np.ndarray]:
        """
        å›¾åƒåˆ°å›¾åƒç”Ÿæˆ
        """
        return self.__call__(image)
    
    def batch_generate(
        self,
        num_images: int = 1,
        show_progress: bool = True,
    ) -> List[Union[Image.Image, torch.Tensor, np.ndarray]]:
        """
        æ‰¹é‡ç”Ÿæˆ
        """
        results = []
        for i in range(num_images):
            result = self.__call__()
            results.append(result)
            
            if show_progress and (i + 1) % 10 == 0:
                print(f"ğŸ“¸ å·²ç”Ÿæˆ {i + 1}/{num_images} å¼ å›¾åƒ")
        
        return results
    
    def change_vae_decode_method(self, method: Literal["normalize", "dynamic", "clamp"]):
        """
        åŠ¨æ€ä¿®æ”¹VAEè§£ç æ–¹æ³•
        """
        self.vae_decode_method = method
        self.stream.vae_decode_method = method
        print(f"ğŸ”„ VAEè§£ç æ–¹æ³•å·²ä¿®æ”¹ä¸º: {method}")
    
    def get_performance_stats(self) -> Dict[str, float]:
        """
        è·å–æ€§èƒ½ç»Ÿè®¡
        """
        return {
            "inference_time_ema": self.stream.get_inference_time(),
        }