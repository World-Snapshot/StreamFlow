import torch, torchvision
from src.scheduler_perflow import PeRFlowScheduler
import time
import os

from diffusers import AutoencoderTiny, StableDiffusionPipeline

# ä½¿ç”¨æˆ‘ä»¬ä¼˜åŒ–çš„StreamFlowè€Œä¸æ˜¯åŸå§‹StreamDiffusion
from src.streamflow import StreamFlow
from src.streamflow.image_utils import postprocess_image

def run_perflow_frame_buffer_test():
    """
    ä½¿ç”¨StreamFlowè¿›è¡ŒPeRFlowå¸§ç¼“å†²æµ‹è¯•
    æµ‹è¯•ä¸åŒframe_buffer_sizeå¯¹æ€§èƒ½å’Œè´¨é‡çš„å½±å“
    """
    print("ğŸš€ PeRFlow + StreamFlow å¸§ç¼“å†²æµ‹è¯•")
    print("=" * 60)
    
    # é…ç½®é€‰é¡¹
    USE_TINY_VAE = False  # è®¾ä¸ºTrueå¯è¿›ä¸€æ­¥åŠ é€Ÿï¼Œä½†ä¼šè½»å¾®å½±å“è´¨é‡
    VAE_DECODE_METHOD = "normalize"  # "normalize" æˆ– "dynamic"
    ACCELERATION = "xformers"  # xformers, tensorrt, none
    
    # æ–°å¢ï¼šå¸§ç¼“å†²å’Œç›¸å…³è®¾ç½®
    FRAME_BUFFER_SIZE = 1  # å¸§ç¼“å†²å¤§å°ï¼š1=æ— ç¼“å†²ï¼Œ2-8=å¤šå¸§ç¼“å†²
    USE_DENOISING_BATCH = True  # æ‰¹é‡é™å™ªï¼šStreamFlowä¸­é»˜è®¤ä¼˜åŒ–
    DO_ADD_NOISE = True  # æ·»åŠ å™ªå£°ï¼šTrue=æ ‡å‡†æ¨¡å¼ï¼ŒFalse=å¿«é€Ÿæ¨¡å¼
    
    print(f"ğŸ”§ é…ç½®:")
    print(f"   VAEç±»å‹: {'TinyVAE' if USE_TINY_VAE else 'åŸå§‹VAE'}")
    print(f"   è§£ç æ–¹æ³•: {VAE_DECODE_METHOD}")
    print(f"   åŠ é€Ÿæ–¹æ³•: {ACCELERATION}")
    print(f"   å¸§ç¼“å†²å¤§å°: {FRAME_BUFFER_SIZE} ({'æ— ç¼“å†²' if FRAME_BUFFER_SIZE == 1 else f'{FRAME_BUFFER_SIZE}å¸§ç¼“å†²'})")
    print(f"   æ‰¹é‡é™å™ª: {'å¯ç”¨(StreamFlowä¼˜åŒ–)' if USE_DENOISING_BATCH else 'ç¦ç”¨'}")
    print(f"   æ·»åŠ å™ªå£°: {'å¯ç”¨' if DO_ADD_NOISE else 'ç¦ç”¨'}")
    
    # å¸§ç¼“å†²è¯´æ˜
    if FRAME_BUFFER_SIZE > 1:
        print(f"   ğŸ’¡ å¸§ç¼“å†²æ•ˆæœ: å¯èƒ½æé«˜æµå¼ç”Ÿæˆçš„æµç•…åº¦å’Œä¸€è‡´æ€§")
    else:
        print(f"   ğŸ’¡ æ— å¸§ç¼“å†²: æ¯å¸§ç‹¬ç«‹ç”Ÿæˆï¼Œå»¶è¿Ÿæœ€ä½")
    
    # åŠ è½½PeRFlowæ¨¡å‹
    print(f"\nğŸ“¦ åŠ è½½PeRFlowæ¨¡å‹...")
    pipe = StableDiffusionPipeline.from_pretrained(
        "hansyan/perflow-sd15-dreamshaper", 
        torch_dtype=torch.float16
    )
    
    # è®¾ç½®PeRFlowè°ƒåº¦å™¨
    pipe.scheduler = PeRFlowScheduler.from_config(
        pipe.scheduler.config, 
        prediction_type="diff_eps", 
        num_time_windows=4
    )
    pipe.to("cuda", torch.float16)
    
    # å¯é€‰ï¼šä½¿ç”¨TinyVAEåŠ é€Ÿ
    if USE_TINY_VAE:
        print("ğŸ”„ åŠ è½½TinyVAE...")
        pipe.vae = AutoencoderTiny.from_pretrained("madebyollin/taesd").to(
            device=pipe.device, dtype=pipe.dtype
        )
    
    # åˆ›å»ºStreamFlowï¼ˆæˆ‘ä»¬çš„ä¼˜åŒ–ç‰ˆæœ¬ï¼‰- å¯ç”¨å¸§ç¼“å†²
    stream = StreamFlow(
        pipe,
        t_index_list=[0, 12, 24, 49],  # PeRFlowçš„4ä¸ªæ—¶é—´æ­¥ [0, 1, 2, 3]ï¼Œä½¿ç”¨49è¿™ç§æ—¶é—´æ­¥ä¼¼ä¹å¯ä»¥æå‡è´¨é‡ï¼š[0, 12, 24, 49]
        torch_dtype=torch.float16,
        frame_buffer_size=FRAME_BUFFER_SIZE,  # å…³é”®ï¼šå¯ç”¨å¸§ç¼“å†²
        cfg_type="full",
        use_original_scheduler=True,  # ä½¿ç”¨PeRFlowåŸç”Ÿè°ƒåº¦å™¨
        vae_decode_method=VAE_DECODE_METHOD,  # ä¼˜åŒ–çš„è§£ç æ–¹æ³•
        do_add_noise=DO_ADD_NOISE,  # æ˜¯å¦æ·»åŠ å™ªå£°
    )
    
    # å¯ç”¨åŠ é€Ÿ
    if ACCELERATION == "xformers":
        pipe.enable_xformers_memory_efficient_attention()
        print("âš¡ å¯ç”¨xformersåŠ é€Ÿ")
    
    # æµ‹è¯•æç¤ºè¯
    prompts_list = ["A man with brown skin, a beard, and dark eyes"]
    prompt = "RAW photo, 8k uhd, dslr, high quality, film grain, highly detailed, masterpiece; " + prompts_list[0]
    neg_prompt = "distorted, blur, smooth, low-quality, warm, haze, over-saturated, high-contrast, out of focus, dark"
    
    print(f"\nğŸ“ æç¤ºè¯: {prompt}")
    
    # é¢„çƒ­é˜¶æ®µ
    print(f"\nğŸ”¥ é¢„çƒ­ä¸­...")
    generator = torch.Generator("cuda").manual_seed(1024)
    
    # å‡†å¤‡StreamFlow
    stream.prepare(prompt, neg_prompt, num_inference_steps=4, guidance_scale=7.5)
    
    # é¢„çƒ­ç”Ÿæˆ
    for i in range(10):
        _ = stream.txt2img()
        if i % 3 == 0:
            print(f"   é¢„çƒ­ {i+1}/10")
    
    print("âœ… é¢„çƒ­å®Œæˆ")
    
    # å¸§ç¼“å†²æµ‹è¯•è®¾ç½®
    output_dir = f"test22_frame_buffer_{FRAME_BUFFER_SIZE}_output"
    os.makedirs(output_dir, exist_ok=True)
    
    # æµ‹è¯•ä¸åŒçš„ç”Ÿæˆæ¨¡å¼
    test_modes = [
        {"name": "txt2img", "iterations": 50, "description": "æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ"},
        {"name": "sequence", "iterations": 30, "description": "åºåˆ—ç”Ÿæˆï¼ˆæ¨¡æ‹Ÿè§†é¢‘å¸§ï¼‰"},
    ]
    
    all_results = {}
    
    for mode_config in test_modes:
        mode_name = mode_config["name"]
        iterations = mode_config["iterations"]
        description = mode_config["description"]
        
        print(f"\nğŸ¨ å¼€å§‹{description}æµ‹è¯• ({mode_name})...")
        print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ”¢ ç”Ÿæˆæ¬¡æ•°: {iterations}")
        
        results = []
        quality_samples = []  # ä¿å­˜ä¸€äº›æ ·æœ¬ç”¨äºè´¨é‡æ£€æŸ¥
        
        # ç”Ÿæˆå¾ªç¯
        for i in range(iterations):
            torch.cuda.synchronize()
            start_time = time.time()
            
            if mode_name == "txt2img":
                # æ ‡å‡†txt2imgç”Ÿæˆ
                sample = stream.txt2img()
            elif mode_name == "sequence":
                # åºåˆ—ç”Ÿæˆï¼šä½¿ç”¨å‰ä¸€å¸§ä½œä¸ºè¾“å…¥ï¼ˆæ¨¡æ‹Ÿè§†é¢‘ï¼‰
                if i == 0:
                    # ç¬¬ä¸€å¸§ä»txt2imgå¼€å§‹
                    sample = stream.txt2img()
                else:
                    # åç»­å¸§ä½¿ç”¨img2imgï¼ˆå¦‚æœæ”¯æŒçš„è¯ï¼‰
                    sample = stream.txt2img()  # æš‚æ—¶è¿˜æ˜¯ç”¨txt2img
            
            torch.cuda.synchronize()
            elapsed = time.time() - start_time
            results.append(elapsed)
            
            # è®¡ç®—FPS
            img_per_sec = 1 / elapsed
            avg_fps = len(results) / sum(results)
            
            # ä¿å­˜å›¾åƒ
            output_path = os.path.join(output_dir, f"{mode_name}_{i:06d}.png")
            torchvision.utils.save_image(sample, output_path)
            
            # ä¿å­˜ä¸€äº›æ ·æœ¬ç”¨äºè´¨é‡å¯¹æ¯”
            if i in [0, 5, 10, 15, 20, iterations-1]:
                quality_samples.append((i, sample.clone()))
            
            # æ˜¾ç¤ºè¿›åº¦
            if i % 10 == 0 or i < 5:
                print(f"ğŸ–¼ï¸  {mode_name} {i+1:3d}/{iterations} | "
                      f"FPS: {img_per_sec:6.2f} | "
                      f"å¹³å‡FPS: {avg_fps:6.2f} | "
                      f"ç”¨æ—¶: {elapsed:.3f}s")
        
        # æ¨¡å¼ç»Ÿè®¡
        if results:
            avg_time = sum(results) / len(results)
            total_fps = len(results) / sum(results)
            min_time = min(results)
            max_time = max(results)
            
            print(f"\nğŸ“Š {description}æ€§èƒ½ç»Ÿè®¡")
            print("-" * 50)
            print(f"æ€»å›¾åƒæ•°:     {len(results)}")
            print(f"å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.3f}s")
            print(f"æœ€å¿«æ—¶é—´:     {min_time:.3f}s ({1/min_time:.2f} FPS)")
            print(f"æœ€æ…¢æ—¶é—´:     {max_time:.3f}s ({1/max_time:.2f} FPS)")
            print(f"å¹³å‡FPS:      {total_fps:.2f}")
            print(f"æ€»ç”¨æ—¶:       {sum(results):.2f}s")
            
            all_results[mode_name] = {
                "avg_time": avg_time,
                "total_fps": total_fps,
                "min_time": min_time,
                "max_time": max_time,
                "total_images": len(results)
            }
        
        # ç”Ÿæˆè´¨é‡æ ·æœ¬ç½‘æ ¼
        if quality_samples:
            print(f"ğŸ¨ ç”Ÿæˆ{mode_name}è´¨é‡æ ·æœ¬ç½‘æ ¼...")
            # åˆ›å»ºæ ·æœ¬ç½‘æ ¼
            sample_tensors = [sample for _, sample in quality_samples]
            grid = torchvision.utils.make_grid(
                torch.cat(sample_tensors, dim=0), 
                nrow=3, 
                padding=2, 
                normalize=False
            )
            
            grid_path = os.path.join(output_dir, f"{mode_name}_quality_samples_grid.png")
            torchvision.utils.save_image(grid, grid_path)
            print(f"âœ… {mode_name}è´¨é‡æ ·æœ¬ç½‘æ ¼å·²ä¿å­˜: {grid_path}")
        
        quality_samples = []  # æ¸…ç©ºå‡†å¤‡ä¸‹ä¸€ä¸ªæ¨¡å¼
    
    # æ•´ä½“æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ† å¸§ç¼“å†²æ€§èƒ½æ€»ç»“")
    print("=" * 60)
    print(f"å¸§ç¼“å†²å¤§å°: {FRAME_BUFFER_SIZE}")
    print("-" * 60)
    print(f"{'æ¨¡å¼':<15} {'å¹³å‡FPS':<10} {'æœ€å¿«FPS':<10} {'å›¾åƒæ•°':<8}")
    print("-" * 60)
    
    for mode_name, stats in all_results.items():
        fastest_fps = 1.0 / stats["min_time"]
        print(f"{mode_name:<15} {stats['total_fps']:<10.2f} {fastest_fps:<10.2f} {stats['total_images']:<8}")
    
    # å¸§ç¼“å†²æ•ˆæœåˆ†æ
    print(f"\nğŸ’¡ å¸§ç¼“å†²æ•ˆæœåˆ†æ:")
    print(f"   ğŸ“Š å½“å‰é…ç½® (buffer_size={FRAME_BUFFER_SIZE}):")
    
    if FRAME_BUFFER_SIZE == 1:
        print(f"      - æ— ç¼“å†²æ¨¡å¼ï¼šæ¯å¸§ç‹¬ç«‹ç”Ÿæˆï¼Œå»¶è¿Ÿæœ€ä½")
        print(f"      - é€‚åˆï¼šå•å¼ å›¾åƒç”Ÿæˆã€æœ€ä½å»¶è¿Ÿéœ€æ±‚")
    elif FRAME_BUFFER_SIZE <= 4:
        print(f"      - å°ç¼“å†²æ¨¡å¼ï¼šå¹³è¡¡å»¶è¿Ÿå’Œæµç•…åº¦")
        print(f"      - é€‚åˆï¼šå®æ—¶åº”ç”¨ã€è½»é‡çº§æµå¼ç”Ÿæˆ")
    else:
        print(f"      - å¤§ç¼“å†²æ¨¡å¼ï¼šæ›´é«˜çš„æµç•…åº¦ï¼Œä½†å»¶è¿Ÿå¢åŠ ")
        print(f"      - é€‚åˆï¼šé«˜è´¨é‡è§†é¢‘ç”Ÿæˆã€æ‰¹é‡å¤„ç†")
    
    # ä¸ä¸åŒbuffer sizeçš„å¯¹æ¯”å»ºè®®
    print(f"\nğŸ”„ å»ºè®®æµ‹è¯•ä¸åŒå¸§ç¼“å†²å¤§å°:")
    print(f"   - buffer_size=1: æœ€ä½å»¶è¿Ÿ")
    print(f"   - buffer_size=2: è½»é‡çº§æµå¼")
    print(f"   - buffer_size=4: å¹³è¡¡æ¨¡å¼ï¼ˆå½“å‰ï¼‰")
    print(f"   - buffer_size=8: é«˜æµç•…åº¦")
    
    print(f"\nğŸ‰ å¸§ç¼“å†²æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ æ‰€æœ‰å›¾åƒå·²ä¿å­˜åˆ°: {output_dir}")
    print(f"ğŸ” è¯·æ£€æŸ¥ä¸åŒæ¨¡å¼çš„å›¾åƒè´¨é‡å’Œè¿ç»­æ€§")
    
    return {
        "frame_buffer_size": FRAME_BUFFER_SIZE,
        "results": all_results,
        "output_dir": output_dir,
        "config": {
            "use_tiny_vae": USE_TINY_VAE,
            "vae_decode_method": VAE_DECODE_METHOD,
            "acceleration": ACCELERATION,
            "use_denoising_batch": USE_DENOISING_BATCH,
            "do_add_noise": DO_ADD_NOISE
        }
    }


if __name__ == "__main__":
    # è¿è¡Œå¸§ç¼“å†²æµ‹è¯•
    results = run_perflow_frame_buffer_test()
    
    print(f"\nğŸ† å¸§ç¼“å†²æµ‹è¯•æ€»ç»“:")
    print(f"å¸§ç¼“å†²å¤§å°: {results['frame_buffer_size']}")
    print(f"æµ‹è¯•ç»“æœ: {results['results']}")
    print(f"é…ç½®: {results['config']}")