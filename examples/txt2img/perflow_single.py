import os
import sys
from typing import Literal, Dict, Optional

import fire

sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))

from utils.wrapper_perflow import PeRFlowWrapper

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))


def main(
    output: str = os.path.join(CURRENT_DIR, "..", "..", "images", "outputs", "perflow_output.png"),
    model_id_or_path: str = "hansyan/perflow-sd15-dreamshaper",
    lora_dict: Optional[Dict[str, float]] = None,
    prompt: str = "RAW photo, 8k uhd, dslr, high quality, film grain, highly detailed, masterpiece; A man with brown skin, a beard, and dark eyes",
    negative_prompt: str = "distorted, blur, smooth, low-quality, warm, haze, over-saturated, high-contrast, out of focus, dark",
    width: int = 512,
    height: int = 512,
    acceleration: Literal["none", "xformers", "tensorrt"] = "xformers",
    vae_decode_method: Literal["normalize", "dynamic", "clamp"] = "normalize",
    use_tiny_vae: bool = False,
    num_inference_steps: int = 4,
    guidance_scale: float = 7.5,
    seed: int = 1024,
    num_images: int = 1,
):
    """
    PeRFlowé«˜è´¨é‡å›¾åƒç”Ÿæˆ

    Parameters
    ----------
    output : str, optional
        è¾“å‡ºå›¾åƒæ–‡ä»¶è·¯å¾„
    model_id_or_path : str
        PeRFlowæ¨¡å‹è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨dreamshaperç‰ˆæœ¬
    lora_dict : Optional[Dict[str, float]], optional
        LoRAå­—å…¸ï¼Œé”®ä¸ºLoRAåç§°ï¼Œå€¼ä¸ºç¼©æ”¾å› å­
        ä¾‹å¦‚: {'LoRA_1': 0.5, 'LoRA_2': 0.7}
    prompt : str
        æ­£å‘æç¤ºè¯
    negative_prompt : str
        è´Ÿå‘æç¤ºè¯
    width : int, optional
        å›¾åƒå®½åº¦ï¼Œé»˜è®¤512
    height : int, optional
        å›¾åƒé«˜åº¦ï¼Œé»˜è®¤512
    acceleration : Literal["none", "xformers", "tensorrt"]
        åŠ é€Ÿæ–¹æ³•ï¼Œæ¨èxformers
    vae_decode_method : Literal["normalize", "dynamic", "clamp"]
        VAEè§£ç æ–¹æ³•ï¼š
        - "normalize": æ ‡å‡†å½’ä¸€åŒ–ï¼Œæ¨èç”¨äºä¿æŒè´¨é‡
        - "dynamic": åŠ¨æ€å½’ä¸€åŒ–ï¼Œæœ€å¤§åŠ¨æ€èŒƒå›´ä½†å¯èƒ½æœ‰è‰²å
        - "clamp": ç›´æ¥æˆªæ–­ï¼Œä¼šåæš—
    use_tiny_vae : bool, optional
        æ˜¯å¦ä½¿ç”¨TinyVAEåŠ é€Ÿï¼ŒFalseä¿è¯æœ€ä½³è´¨é‡
    num_inference_steps : int, optional
        æ¨ç†æ­¥æ•°ï¼ŒPeRFlowæ¨è4æ­¥
    guidance_scale : float, optional
        CFGå¼•å¯¼ç¼©æ”¾ï¼Œé»˜è®¤7.5
    seed : int, optional
        éšæœºç§å­ï¼Œé»˜è®¤1024
    num_images : int, optional
        ç”Ÿæˆå›¾åƒæ•°é‡ï¼Œé»˜è®¤1å¼ 
    """
    
    print("ğŸ¨ PeRFlowé«˜è´¨é‡å›¾åƒç”Ÿæˆ")
    print(f"ğŸ“ æç¤ºè¯: {prompt}")
    print(f"ğŸ”§ VAEè§£ç æ–¹æ³•: {vae_decode_method}")
    print(f"ğŸ“ å°ºå¯¸: {width}x{height}")
    print(f"ğŸ¯ ç”Ÿæˆæ•°é‡: {num_images}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(output), exist_ok=True)
    
    # åˆå§‹åŒ–PeRFlowåŒ…è£…å™¨
    wrapper = PeRFlowWrapper(
        model_id_or_path=model_id_or_path,
        t_index_list=[0, 1, 2, 3],  # PeRFlowæ ‡å‡†4æ­¥
        lora_dict=lora_dict,
        mode="txt2img",
        output_type="pil",
        vae_decode_method=vae_decode_method,
        device="cuda",
        dtype=torch.float16,
        width=width,
        height=height,
        warmup=5,
        acceleration=acceleration,
        use_tiny_vae=use_tiny_vae,
        seed=seed,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
    )
    
    # å‡†å¤‡æ¨ç†
    wrapper.prepare(
        prompt=prompt,
        negative_prompt=negative_prompt,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
    )
    
    # ç”Ÿæˆå›¾åƒ
    import time
    start_time = time.time()
    
    if num_images == 1:
        # å•å¼ å›¾åƒ
        image = wrapper.txt2img()
        image.save(output)
        print(f"âœ… å›¾åƒå·²ä¿å­˜: {output}")
    else:
        # æ‰¹é‡ç”Ÿæˆ
        images = wrapper.batch_generate(num_images=num_images, show_progress=True)
        
        # ä¿å­˜å¤šå¼ å›¾åƒ
        base_name = os.path.splitext(output)[0]
        ext = os.path.splitext(output)[1]
        
        for i, image in enumerate(images):
            if num_images == 1:
                save_path = output
            else:
                save_path = f"{base_name}_{i+1:03d}{ext}"
            image.save(save_path)
            print(f"âœ… å›¾åƒå·²ä¿å­˜: {save_path}")
    
    generation_time = time.time() - start_time
    
    # æ€§èƒ½ç»Ÿè®¡
    stats = wrapper.get_performance_stats()
    print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
    print(f"   æ€»ç”¨æ—¶: {generation_time:.2f}s")
    print(f"   å•å¼ å¹³å‡: {generation_time/num_images:.2f}s")
    print(f"   å¹³å‡FPS: {num_images/generation_time:.2f}")
    print(f"   æ¨ç†æ—¶é—´EMA: {stats['inference_time_ema']:.3f}s")


if __name__ == "__main__":
    import torch
    fire.Fire(main)