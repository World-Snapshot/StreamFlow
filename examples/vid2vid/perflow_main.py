import os
import sys
import time
from pathlib import Path
from typing import Literal, Dict, Optional

import cv2
import fire
import numpy as np
import torch
from PIL import Image

sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))

from utils.wrapper_perflow import PeRFlowWrapper

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))


def process_video(
    input_video: str,
    output_video: str = os.path.join(CURRENT_DIR, "..", "..", "images", "outputs", "perflow_video_output.mp4"),
    model_id_or_path: str = "hansyan/perflow-sd15-dreamshaper", 
    prompt: str = "anime style, vibrant colors, detailed",
    negative_prompt: str = "blurry, low quality, distorted",
    vae_decode_method: Literal["normalize", "dynamic", "clamp"] = "normalize",
    use_tiny_vae: bool = True,  # è§†é¢‘å¤„ç†æ¨èä½¿ç”¨TinyVAEåŠ é€Ÿ
    acceleration: Literal["none", "xformers", "tensorrt"] = "xformers",
    guidance_scale: float = 7.5,
    strength: float = 0.8,  # img2imgå¼ºåº¦
    seed: int = 42,
    fps: Optional[int] = None,  # è¾“å‡ºfpsï¼ŒNoneåˆ™ä½¿ç”¨è¾“å…¥è§†é¢‘çš„fps
    max_frames: Optional[int] = None,  # æœ€å¤§å¤„ç†å¸§æ•°ï¼Œç”¨äºæµ‹è¯•
    skip_frames: int = 1,  # è·³å¸§å¤„ç†ï¼Œ1è¡¨ç¤ºå¤„ç†æ¯ä¸€å¸§
    output_width: int = 512,
    output_height: int = 512,
):
    """
    PeRFlowè§†é¢‘åˆ°è§†é¢‘å¤„ç†
    
    Parameters
    ----------
    input_video : str
        è¾“å…¥è§†é¢‘è·¯å¾„
    output_video : str
        è¾“å‡ºè§†é¢‘è·¯å¾„
    model_id_or_path : str
        PeRFlowæ¨¡å‹è·¯å¾„
    prompt : str
        é£æ ¼åŒ–æç¤ºè¯
    negative_prompt : str
        è´Ÿå‘æç¤ºè¯
    vae_decode_method : str
        VAEè§£ç æ–¹æ³•ï¼Œæ¨è"normalize"
    use_tiny_vae : bool
        è§†é¢‘å¤„ç†æ¨èTrueä»¥æé«˜é€Ÿåº¦
    acceleration : str
        åŠ é€Ÿæ–¹æ³•
    guidance_scale : float
        CFGå¼•å¯¼å¼ºåº¦
    strength : float
        img2imgå˜æ¢å¼ºåº¦ï¼Œ0.0-1.0
    seed : int
        éšæœºç§å­
    fps : int
        è¾“å‡ºè§†é¢‘å¸§ç‡ï¼ŒNoneä½¿ç”¨è¾“å…¥è§†é¢‘å¸§ç‡
    max_frames : int
        æœ€å¤§å¤„ç†å¸§æ•°ï¼Œç”¨äºæµ‹è¯•
    skip_frames : int
        è·³å¸§å¤„ç†é—´éš”
    output_width : int
        è¾“å‡ºå®½åº¦
    output_height : int
        è¾“å‡ºé«˜åº¦
    """
    
    print("ğŸ¬ PeRFlowè§†é¢‘å¤„ç†å¼€å§‹")
    print(f"ğŸ“¹ è¾“å…¥è§†é¢‘: {input_video}")
    print(f"ğŸ’¾ è¾“å‡ºè§†é¢‘: {output_video}")
    print(f"ğŸ¨ é£æ ¼æç¤º: {prompt}")
    print(f"ğŸ”§ VAEè§£ç : {vae_decode_method}")
    print(f"âš¡ ä½¿ç”¨TinyVAE: {use_tiny_vae}")
    
    # æ£€æŸ¥è¾“å…¥è§†é¢‘
    if not os.path.exists(input_video):
        raise FileNotFoundError(f"è¾“å…¥è§†é¢‘ä¸å­˜åœ¨: {input_video}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(output_video), exist_ok=True)
    
    # è¯»å–è¾“å…¥è§†é¢‘ä¿¡æ¯
    cap = cv2.VideoCapture(input_video)
    input_fps = int(cap.get(cv2.CAP_PROP_FPS))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    input_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    input_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    print(f"ğŸ“Š è¾“å…¥è§†é¢‘ä¿¡æ¯:")
    print(f"   åˆ†è¾¨ç‡: {input_width}x{input_height}")
    print(f"   å¸§ç‡: {input_fps} FPS")
    print(f"   æ€»å¸§æ•°: {total_frames}")
    
    if fps is None:
        fps = input_fps
    
    if max_frames:
        process_frames = min(max_frames, total_frames)
    else:
        process_frames = total_frames
    
    actual_process_frames = process_frames // skip_frames
    print(f"   å°†å¤„ç†: {actual_process_frames} å¸§ (è·³å¸§é—´éš”: {skip_frames})")
    
    # åˆå§‹åŒ–PeRFlowåŒ…è£…å™¨
    wrapper = PeRFlowWrapper(
        model_id_or_path=model_id_or_path,
        t_index_list=[0, 1, 2, 3],
        mode="img2img",  # è§†é¢‘å¤„ç†ä½¿ç”¨img2imgæ¨¡å¼
        output_type="pil",
        vae_decode_method=vae_decode_method,
        device="cuda",
        dtype=torch.float16,
        width=output_width,
        height=output_height,
        warmup=3,
        acceleration=acceleration,
        use_tiny_vae=use_tiny_vae,
        seed=seed,
        num_inference_steps=4,
        guidance_scale=guidance_scale,
    )
    
    # å‡†å¤‡æ¨ç†
    wrapper.prepare(
        prompt=prompt,
        negative_prompt=negative_prompt,
        guidance_scale=guidance_scale,
    )
    
    # è®¾ç½®è§†é¢‘å†™å…¥å™¨
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video, fourcc, fps, (output_width, output_height))
    
    # å¤„ç†è§†é¢‘å¸§
    frame_count = 0
    processed_count = 0
    start_time = time.time()
    
    print(f"\nğŸš€ å¼€å§‹å¤„ç†è§†é¢‘å¸§...")
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_count >= process_frames:
                break
            
            # è·³å¸§å¤„ç†
            if frame_count % skip_frames == 0:
                # é¢„å¤„ç†å¸§
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame_pil = Image.fromarray(frame_rgb)
                frame_pil = frame_pil.resize((output_width, output_height), Image.Resampling.LANCZOS)
                
                # PeRFlowå¤„ç†
                frame_start = time.time()
                processed_pil = wrapper.img2img(frame_pil)
                frame_time = time.time() - frame_start
                
                # è½¬æ¢å›OpenCVæ ¼å¼
                processed_np = np.array(processed_pil)
                processed_bgr = cv2.cvtColor(processed_np, cv2.COLOR_RGB2BGR)
                
                # å†™å…¥è§†é¢‘
                out.write(processed_bgr)
                
                processed_count += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                if processed_count % 10 == 0 or processed_count <= 5:
                    elapsed = time.time() - start_time
                    avg_fps = processed_count / elapsed if elapsed > 0 else 0
                    eta = (actual_process_frames - processed_count) / avg_fps if avg_fps > 0 else 0
                    
                    print(f"ğŸ“¸ å¸§ {processed_count:4d}/{actual_process_frames} "
                          f"| å¤„ç†ç”¨æ—¶: {frame_time:.3f}s "
                          f"| å¹³å‡FPS: {avg_fps:.2f} "
                          f"| ETA: {eta:.1f}s")
            
            frame_count += 1
    
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†")
    
    finally:
        cap.release()
        out.release()
        cv2.destroyAllWindows()
    
    total_time = time.time() - start_time
    
    # æ€§èƒ½ç»Ÿè®¡
    print(f"\nâœ… è§†é¢‘å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
    print(f"   å¤„ç†å¸§æ•°: {processed_count}")
    print(f"   æ€»ç”¨æ—¶: {total_time:.2f}s")
    print(f"   å¹³å‡æ¯å¸§: {total_time/processed_count:.3f}s")
    print(f"   å¤„ç†FPS: {processed_count/total_time:.2f}")
    print(f"ğŸ’¾ è¾“å‡ºè§†é¢‘: {output_video}")


def process_webcam(
    output_video: str = os.path.join(CURRENT_DIR, "..", "..", "images", "outputs", "perflow_webcam_output.mp4"),
    model_id_or_path: str = "hansyan/perflow-sd15-dreamshaper",
    prompt: str = "anime style, vibrant colors, detailed",
    negative_prompt: str = "blurry, low quality, distorted", 
    vae_decode_method: Literal["normalize", "dynamic", "clamp"] = "normalize",
    use_tiny_vae: bool = True,
    duration: int = 30,  # å½•åˆ¶æ—¶é•¿ï¼ˆç§’ï¼‰
    display: bool = True,  # æ˜¯å¦æ˜¾ç¤ºå®æ—¶é¢„è§ˆ
):
    """
    å®æ—¶æ‘„åƒå¤´å¤„ç†
    """
    print("ğŸ“¹ PeRFlowå®æ—¶æ‘„åƒå¤´å¤„ç†")
    print(f"â±ï¸  å½•åˆ¶æ—¶é•¿: {duration}ç§’")
    print(f"ğŸ¨ é£æ ¼: {prompt}")
    print("æŒ‰ 'q' æå‰é€€å‡º")
    
    # åˆå§‹åŒ–æ‘„åƒå¤´
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        raise RuntimeError("æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
    
    # è®¾ç½®æ‘„åƒå¤´åˆ†è¾¨ç‡
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    fps = 30
    
    # åˆå§‹åŒ–PeRFlow
    wrapper = PeRFlowWrapper(
        model_id_or_path=model_id_or_path,
        t_index_list=[0, 1, 2, 3],
        mode="img2img",
        output_type="pil",
        vae_decode_method=vae_decode_method,
        use_tiny_vae=use_tiny_vae,
        width=512,
        height=512,
        warmup=5,
    )
    
    wrapper.prepare(prompt=prompt, negative_prompt=negative_prompt)
    
    # è§†é¢‘å†™å…¥å™¨
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video, fourcc, fps, (512, 512))
    
    start_time = time.time()
    frame_count = 0
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # æ£€æŸ¥æ—¶é—´é™åˆ¶
            elapsed = time.time() - start_time
            if elapsed >= duration:
                break
            
            # å¤„ç†å¸§
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_pil = Image.fromarray(frame_rgb).resize((512, 512))
            
            processed_pil = wrapper.img2img(frame_pil)
            processed_np = np.array(processed_pil)
            processed_bgr = cv2.cvtColor(processed_np, cv2.COLOR_RGB2BGR)
            
            # ä¿å­˜åˆ°è§†é¢‘
            out.write(processed_bgr)
            
            # æ˜¾ç¤ºé¢„è§ˆ
            if display:
                # å¹¶æ’æ˜¾ç¤ºåŸå§‹å’Œå¤„ç†åçš„å¸§
                original_resized = cv2.resize(frame, (512, 512))
                combined = np.hstack([original_resized, processed_bgr])
                cv2.imshow('PeRFlow Real-time (Original | Processed)', combined)
                
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
            
            frame_count += 1
            
            if frame_count % 30 == 0:
                avg_fps = frame_count / elapsed if elapsed > 0 else 0
                print(f"ğŸ“¸ å¤„ç†äº† {frame_count} å¸§ | å¹³å‡FPS: {avg_fps:.2f}")
    
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    
    finally:
        cap.release()
        out.release()
        cv2.destroyAllWindows()
    
    total_time = time.time() - start_time
    print(f"\nâœ… å®æ—¶å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š ç»Ÿè®¡: {frame_count} å¸§ï¼Œ{total_time:.1f}sï¼Œå¹³å‡ {frame_count/total_time:.2f} FPS")
    print(f"ğŸ’¾ è¾“å‡º: {output_video}")


if __name__ == "__main__":
    fire.Fire({
        "video": process_video,
        "webcam": process_webcam,
    })