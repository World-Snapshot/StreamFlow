#!/usr/bin/env python3
"""
Test58: ç®€æ´çš„é«˜æ€§èƒ½ç”Ÿæˆå™¨

åŸºäºtest57çš„æ‰€æœ‰åŠŸèƒ½ï¼Œä½†å»æ‰å®éªŒæ€§ä»£ç ï¼Œä¿æŒç®€æ´ï¼š
- ğŸš€ TensorRTä¿®å¤ç‰ˆï¼ˆè‡ªåŠ¨å¤„ç†æ—¶é—´æ­¥å…¼å®¹æ€§ï¼‰
- âš¡ æµæ°´çº¿æ‰¹é‡å»å™ª
- ğŸ”§ ç®€å•é…ç½®ï¼Œç›´æ¥ç”Ÿæˆ
- ğŸ“Š æ¸…æ™°çš„æ€§èƒ½æŠ¥å‘Š
- ğŸ“„ æ”¯æŒYAMLé…ç½®æ–‡ä»¶
"""

import torch, torchvision
from src.scheduler_perflow import PeRFlowScheduler
import time
import os
import sys
import yaml
from pathlib import Path

from diffusers import AutoencoderTiny, StableDiffusionPipeline
from src.streamflow.pipeline_batch_pipeline import PipelineBatchStreamFlow


def load_config_from_yaml(yaml_path):
    """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
    with open(yaml_path, 'r') as f:
        config = yaml.safe_load(f)
    return config


def add_tensorrt_timestep_compatibility(stream):
    """
    ğŸš€ å…³é”®ä¿®å¤ï¼šæ·»åŠ TensorRTæ—¶é—´æ­¥å…¼å®¹æ€§
    
    é—®é¢˜ï¼šTensorRTä¸èƒ½å¤„ç†ä¸åŒæ—¶é—´æ­¥ [1000, 750, 500, 250]
    è§£å†³ï¼šåœ¨æ£€æµ‹åˆ°ä¸åŒæ—¶é—´æ­¥æ—¶ï¼Œåˆ†è§£ä¸ºå•ç‹¬è°ƒç”¨
    """
    if not hasattr(stream.unet, 'forward'):
        print("âš ï¸  UNetæ²¡æœ‰forwardæ–¹æ³•ï¼Œè·³è¿‡å…¼å®¹æ€§ä¿®å¤")
        return None
    
    original_forward = stream.unet.forward
    
    def tensorrt_compatible_forward(sample, timestep, encoder_hidden_states, **kwargs):
        """
        TensorRTå…¼å®¹çš„forwardæ–¹æ³•
        è‡ªåŠ¨å¤„ç†ä¸åŒæ—¶é—´æ­¥é—®é¢˜
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸åŒæ—¶é—´æ­¥
        if isinstance(timestep, torch.Tensor) and timestep.dim() > 0 and len(timestep) > 1:
            unique_timesteps = torch.unique(timestep)
            if len(unique_timesteps) > 1:
                # ğŸ¯ æ£€æµ‹åˆ°ä¸åŒæ—¶é—´æ­¥ï¼Œåˆ†è§£å¤„ç†
                batch_size = sample.shape[0]
                results = []
                for i in range(batch_size):
                    single_sample = sample[i:i+1]
                    single_timestep = timestep[i:i+1]
                    single_encoder_states = encoder_hidden_states[i:i+1]
                    
                    single_result = original_forward(
                        single_sample, single_timestep, single_encoder_states, **kwargs
                    )
                    results.append(single_result)
                
                # é‡æ–°ç»„è£…ç»“æœ
                if isinstance(results[0], tuple):
                    assembled = []
                    for i in range(len(results[0])):
                        assembled.append(torch.cat([r[i] for r in results], dim=0))
                    return tuple(assembled)
                else:
                    return torch.cat(results, dim=0)
        
        # ç›¸åŒæ—¶é—´æ­¥æˆ–å•ä¸ªæ—¶é—´æ­¥ï¼Œç›´æ¥è°ƒç”¨TensorRT
        return original_forward(sample, timestep, encoder_hidden_states, **kwargs)
    
    # åº”ç”¨å…¼å®¹æ€§ä¿®å¤
    stream.unet.forward = tensorrt_compatible_forward
    print("âœ… TensorRTæ—¶é—´æ­¥å…¼å®¹æ€§å·²æ·»åŠ ")
    return original_forward


# ================================
# ğŸ”§ é…ç½®åŒºåŸŸ - æ‰€æœ‰è®¾ç½®éƒ½åœ¨è¿™é‡Œ
# ================================

# ğŸ”§ æ£€æŸ¥æ˜¯å¦é€šè¿‡å‘½ä»¤è¡Œä¼ å…¥YAMLé…ç½®
if len(sys.argv) > 1 and sys.argv[1].endswith('.yaml'):
    CONFIG_PATH = sys.argv[1]
    print(f"ğŸ“„ ä»YAMLåŠ è½½é…ç½®: {CONFIG_PATH}")
    config = load_config_from_yaml(CONFIG_PATH)
    CONFIG_NAME = config.get('name', 'unnamed')

    # ä»YAMLåŠ è½½é…ç½®
    USE_TINY_VAE = config['model']['use_tiny_vae']
    USE_INT8_VAE = config['model']['use_int8_vae']
    ACCELERATION = config['acceleration']['type']
    USE_CUDA_GRAPH = config['acceleration'].get('use_cuda_graph', False)
    ITERATIONS = config['test']['iterations']

    USE_PIPELINE_BATCH = config['pipeline']['use_pipeline_batch']
    FRAME_BUFFER_SIZE = config['pipeline'].get('frame_buffer_size', 1)
    VAE_DECODE_METHOD = config['pipeline'].get('vae_decode_method', 'normalize')
    DO_ADD_NOISE = config['pipeline'].get('do_add_noise', True)
    CFG_TYPE = config['pipeline']['cfg_type']
    GUIDANCE_SCALE = config['pipeline']['guidance_scale']

    USE_DYNAMIC_STEPS = config['denoising']['use_dynamic_steps']
    NUM_INFERENCE_STEPS = config['denoising']['num_inference_steps']

    USE_TENSORRT_COMPATIBILITY = config['tensorrt']['use_compatibility']
    TENSORRT_OPTIMIZATION = config['tensorrt'].get('optimization', {})

    VAE_BATCH_SIZE = config['vae']['batch_size']

    PROMPT_BASE = config['prompts']['base']
    PROMPT_SUBJECT = config['prompts']['subject']
    NEGATIVE_PROMPT = config['prompts']['negative']

    OUTPUT_DIR = os.path.join(config['test']['output_dir'], CONFIG_NAME)
    SEED = config['test']['seed']
    WIDTH = config['test'].get('width', 512)
    HEIGHT = config['test'].get('height', 512)

else:
    # é»˜è®¤é…ç½®ï¼ˆä¿æŒåŸæœ‰çš„ç¡¬ç¼–ç é…ç½®ï¼‰
    CONFIG_NAME = "default"

    # åŸºç¡€é…ç½®ï¼ˆåŸºäºtest43ï¼‰
    USE_TINY_VAE = True              # è®¾ä¸ºTrueå¯è¿›ä¸€æ­¥åŠ é€Ÿï¼Œä½†ä¼šè½»å¾®å½±å“è´¨é‡
    USE_INT8_VAE = False             # ğŸ”¬ å®éªŒæ€§ï¼šINT8é‡åŒ–VAEï¼ˆæ›´å¿«ä½†å¯èƒ½å½±å“è´¨é‡ï¼‰
    ACCELERATION = "xformers"        # "xformers", "tensorrt", "none" - ğŸš€ æµ‹è¯•ä¿®å¤åçš„tensorrt
    USE_CUDA_GRAPH = False           # CUDA Graphsä¼˜åŒ–
    ITERATIONS = 100                 # ç”Ÿæˆå›¾åƒæ•°é‡

    # æµæ°´çº¿é…ç½®
    USE_PIPELINE_BATCH = True        # ğŸš€ å…³é”®æ–°å¢ï¼šçœŸæ­£çš„æ‰¹é‡å»å™ªå¼€å…³ True=æµæ°´çº¿æ‰¹é‡å»å™ªï¼ŒFalse=åŸå§‹StreamFlow
    FRAME_BUFFER_SIZE = 1            # å¸§ç¼“å†²å¤§å°
    VAE_DECODE_METHOD = "normalize"  # VAEè§£ç æ–¹æ³•
    DO_ADD_NOISE = True              # æ·»åŠ å™ªå£°
    CFG_TYPE = "none"               # "none", "full", "self", "initialize" - I usually use none and full
    GUIDANCE_SCALE = 7.5            # CFGå¼ºåº¦

    # å»å™ªæ­¥æ•°é…ç½®
    USE_DYNAMIC_STEPS = False       # ğŸ”§ æ˜¯å¦ä½¿ç”¨åŠ¨æ€æ­¥æ•°
                                    # False=å›ºå®š4æ­¥[0,1,2,3]ï¼ˆè´¨é‡å¥½ï¼Œæ¨èï¼‰
                                    # True=æ ¹æ®NUM_INFERENCE_STEPSåŠ¨æ€ï¼ˆçµæ´»ï¼Œæµ‹è¯•ç”¨ï¼‰
    NUM_INFERENCE_STEPS = 4         # æ¨ç†æ­¥æ•°ï¼ˆä»…å½“USE_DYNAMIC_STEPS=Trueæ—¶ç”Ÿæ•ˆï¼‰

    # TensorRTé«˜çº§é…ç½®
    USE_TENSORRT_COMPATIBILITY = False  # ğŸ”§ TensorRTæ—¶é—´æ­¥å…¼å®¹æ€§å±‚
                                         # False=ç›´æ¥æ‰¹å¤„ç†ï¼ˆæ›´å¿«2fpsï¼Œä½†æ˜¯è½»å¾®æŸå¤±è´¨é‡ï¼‰
                                         # True=æ‹†åˆ†å¤„ç†ä¸åŒæ—¶é—´æ­¥ï¼ˆå®‰å…¨ä½†ä¼šæ…¢2fpsï¼‰
    TENSORRT_OPTIMIZATION = {}       # TensorRTç¼–è¯‘ä¼˜åŒ–é€‰é¡¹

    # VAEä¼˜åŒ–é…ç½®
    VAE_BATCH_SIZE = 1  # ğŸš€ VAEæ‰¹é‡è§£ç ï¼šç´¯ç§¯Nå¼ latentåæ‰¹é‡è§£ç 
                        # 1=é€ä¸ªè§£ç ï¼ˆæ…¢ï¼Œå»¶è¿Ÿä½ï¼‰
                        # 4=æ‰¹é‡è§£ç ï¼ˆå¿«50%+ï¼Œå»¶è¿Ÿç¨é«˜ï¼‰
                        # å»ºè®®ï¼šç¦»çº¿ç”Ÿæˆç”¨4-8ï¼Œå®æ—¶ç”¨1-2

    # æç¤ºè¯
    PROMPT_BASE = "RAW photo, 8k uhd, dslr, high quality, film grain, highly detailed, masterpiece"
    PROMPT_SUBJECT = "A man with brown skin, a beard, and dark eyes"
    NEGATIVE_PROMPT = "distorted, blur, smooth, low-quality, warm, haze, over-saturated, high-contrast, out of focus, dark"

    # è¾“å‡ºé…ç½®
    OUTPUT_DIR = "test59_simple_output"
    SEED = 1024
    WIDTH = 512
    HEIGHT = 512

print(f"ğŸš€ PeRFlowé«˜æ€§èƒ½ç”Ÿæˆå™¨ - é…ç½®: {CONFIG_NAME}")
print("=" * 50)
print(f"ğŸ”§ é…ç½®è¯¦æƒ…:")
vae_desc = "TinyVAE" if USE_TINY_VAE else "åŸå§‹VAE"
if USE_INT8_VAE:
    vae_desc += " + INT8é‡åŒ–"
print(f"   VAE: {vae_desc}")
print(f"   åŠ é€Ÿ: {ACCELERATION}")
print(f"   æµæ°´çº¿æ‰¹é‡: {'âœ…' if USE_PIPELINE_BATCH else 'âŒ'}")
print(f"   ç”Ÿæˆæ•°é‡: {ITERATIONS}")

# ================================
# ğŸ“¦ æ¨¡å‹åŠ è½½
# ================================
print(f"\\nğŸ“¦ åŠ è½½æ¨¡å‹...")

pipe = StableDiffusionPipeline.from_pretrained(
    "hansyan/perflow-sd15-dreamshaper", 
    torch_dtype=torch.float16
)

pipe.scheduler = PeRFlowScheduler.from_config(
    pipe.scheduler.config, 
    prediction_type="diff_eps", 
    num_time_windows=4
)
pipe.to("cuda", torch.float16)
# é‡ç½®æ˜¾å­˜å³°å€¼ç»Ÿè®¡ï¼Œä¾¿äºç›‘æ§
torch.cuda.reset_peak_memory_stats()

if USE_TINY_VAE:
    if USE_INT8_VAE:
        # åŠ è½½é¢„é‡åŒ–çš„INT8 TinyVAE
        from utils.quantization import load_quantized_tinyvae
        pipe.vae = load_quantized_tinyvae(device=pipe.device, dtype=pipe.dtype)
    else:
        # åŠ è½½æ™®é€šTinyVAE
        pipe.vae = AutoencoderTiny.from_pretrained("madebyollin/taesd").to(
            device=pipe.device, dtype=pipe.dtype
        )

# ================================
# ğŸš€ åˆ›å»ºæµæ°´çº¿
# ================================
print("ğŸš€ åˆ›å»ºæµæ°´çº¿...")

# ğŸ”§ æ ¹æ®é…ç½®é€‰æ‹©å»å™ªæ­¥æ•°
if USE_DYNAMIC_STEPS:
    # åŠ¨æ€æ¨¡å¼ï¼šæ ¹æ®NUM_INFERENCE_STEPSç”Ÿæˆ
    t_index_list = list(range(NUM_INFERENCE_STEPS))
    prepare_steps = NUM_INFERENCE_STEPS
    print(f"   å»å™ªæ¨¡å¼: åŠ¨æ€æ­¥æ•°")
    print(f"   å»å™ªæ­¥æ•°: {NUM_INFERENCE_STEPS}")
    print(f"   æ—¶é—´æ­¥ç´¢å¼•: {t_index_list}")
else:
    # å›ºå®šæ¨¡å¼ï¼šä½¿ç”¨é¢„è®¾çš„4æ­¥ï¼ˆè´¨é‡æœ€ä¼˜ï¼‰
    t_index_list = [0, 1, 2, 3]
    prepare_steps = 4
    print(f"   å»å™ªæ¨¡å¼: å›ºå®š4æ­¥ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰")
    print(f"   æ—¶é—´æ­¥ç´¢å¼•: {t_index_list}")

stream = PipelineBatchStreamFlow(
    pipe,
    t_index_list=t_index_list,  # åŠ¨æ€ç”Ÿæˆï¼Œè·ŸéšNUM_INFERENCE_STEPS
    torch_dtype=torch.float16,
    frame_buffer_size=FRAME_BUFFER_SIZE,  # å¸§ç¼“å†²å¤§å°ï¼š1=æ— ç¼“å†²ï¼Œ2-8=å¤šå¸§ç¼“å†²
    cfg_type=CFG_TYPE,  # none, full, self, initialize
    use_pipeline_batch=USE_PIPELINE_BATCH,  # å¯ç”¨æµæ°´çº¿æ‰¹é‡å»å™ª
    vae_decode_method=VAE_DECODE_METHOD,  # "normalize", "dynamic", "clamp"
    do_add_noise=DO_ADD_NOISE,  # æ·»åŠ å™ªå£°ï¼šTrue=æ ‡å‡†æ¨¡å¼ï¼ŒFalse=å¿«é€Ÿæ¨¡å¼
)

# ================================
# âš¡ åŠ é€Ÿè®¾ç½®
# ================================
if ACCELERATION == "xformers":
    pipe.enable_xformers_memory_efficient_attention()
    print("âš¡ xformersåŠ é€Ÿå·²å¯ç”¨")
elif ACCELERATION == "tensorrt":
    print("ğŸš€ å¯ç”¨TensorRTåŠ é€Ÿ...")
    try:
        from src.streamdiffusion.acceleration.tensorrt import accelerate_with_tensorrt
        from src.streamdiffusion.pipeline import StreamDiffusion

        # ğŸ”§ æ ¹æ®USE_PIPELINE_BATCHé€‰æ‹©åˆé€‚çš„ç¼–è¯‘é…ç½®
        if USE_PIPELINE_BATCH:
            # æµæ°´çº¿æ¨¡å¼ï¼šéœ€è¦å¤„ç†4ä¸ªä¸åŒé˜¶æ®µ (batch_size=4)
            compile_use_denoising_batch = True
            compile_max_batch_size = 4
            engine_dir = os.path.join("tensorrt_engines_pipeline_batch", CONFIG_NAME)
        else:
            # æ™®é€šæ¨¡å¼ï¼šé€æ­¥å»å™ª (batch_size=1 æˆ– 2 for CFG)
            compile_use_denoising_batch = False
            compile_max_batch_size = 2 if GUIDANCE_SCALE > 1.0 and CFG_TYPE != "none" else 1
            engine_dir = os.path.join("tensorrt_engines_sequential", CONFIG_NAME)

        os.makedirs(engine_dir, exist_ok=True)
        print(f"   å¼•æ“ç›®å½•: {engine_dir}")
        print(f"   ç¼–è¯‘batch_size: {compile_max_batch_size}")

        temp_stream = StreamDiffusion(
            pipe, t_index_list=t_index_list, torch_dtype=torch.float16,
            frame_buffer_size=1, cfg_type=CFG_TYPE,
            use_denoising_batch=compile_use_denoising_batch,  # ğŸ”§ æ ¹æ®æ¨¡å¼é€‰æ‹©
            width=512, height=512,
        )

        temp_stream.prepare(PROMPT_BASE, NEGATIVE_PROMPT, num_inference_steps=prepare_steps, guidance_scale=GUIDANCE_SCALE)

        print("   ç¼–è¯‘TensorRTå¼•æ“...")
        accelerated_stream = accelerate_with_tensorrt(
            temp_stream, engine_dir=engine_dir,
            max_batch_size=compile_max_batch_size,  # ğŸ”§ åŒ¹é…æ¨ç†æ—¶çš„batch size
            min_batch_size=1,
            use_cuda_graph=False,
        )
        
        stream.unet = accelerated_stream.unet
        stream.vae = accelerated_stream.vae  # ğŸš€ ä½¿ç”¨TensorRTåŠ é€Ÿçš„VAE

        # ğŸš€ åº”ç”¨æ—¶é—´æ­¥å…¼å®¹æ€§ä¿®å¤ï¼ˆå¯é€‰ï¼‰
        if USE_PIPELINE_BATCH and USE_TENSORRT_COMPATIBILITY:
            add_tensorrt_timestep_compatibility(stream)
            print("   âš ï¸  æ—¶é—´æ­¥å…¼å®¹æ€§ä¿®å¤å·²åº”ç”¨ï¼ˆä¼šé™ä½æ€§èƒ½ï¼‰")
        elif USE_PIPELINE_BATCH and not USE_TENSORRT_COMPATIBILITY:
            print("   ğŸš€ å…¼å®¹æ€§å±‚å·²ç¦ç”¨ï¼Œä½¿ç”¨åŸç”ŸTensorRTæ‰¹å¤„ç†")
        
        del temp_stream, accelerated_stream
        torch.cuda.empty_cache()
        print("âœ… TensorRTåŠ é€Ÿå·²å¯ç”¨")
        
    except Exception as e:
        print(f"âŒ TensorRTå¤±è´¥ï¼Œå›é€€åˆ°æ— åŠ é€Ÿ: {e}")
        ACCELERATION = "none"

# ================================
# ğŸ”¥ é¢„çƒ­
# ================================
# é¢„çƒ­é˜¶æ®µ
print(f"\\nğŸ”¥ é¢„çƒ­ä¸­...")

generator = torch.Generator("cuda").manual_seed(SEED)

prompt_text = f"{PROMPT_BASE}; {PROMPT_SUBJECT}"

# å‡†å¤‡StreamFlow
stream.prepare(prompt_text, NEGATIVE_PROMPT, num_inference_steps=prepare_steps, guidance_scale=GUIDANCE_SCALE)

# é¢„çƒ­ç”Ÿæˆ
for i in range(10):
    _ = stream.txt2img()
    if i == 0:
        print("âœ… é¢„çƒ­å®Œæˆ")

# ================================
# ğŸ¨ å›¾åƒç”Ÿæˆ
# ================================
print(f"\\nğŸ¨ å¼€å§‹ç”Ÿæˆ {ITERATIONS} å¼ å›¾åƒ...")
print(f"ğŸ“ æç¤ºè¯: {prompt_text}")

os.makedirs(OUTPUT_DIR, exist_ok=True)
results = []

if VAE_BATCH_SIZE > 1:
    print(f"ğŸš€ VAEæ‰¹é‡è§£ç æ¨¡å¼: batch_size={VAE_BATCH_SIZE}")

    # æ‰¹é‡VAEè§£ç æ¨¡å¼
    latent_buffer = []
    buffer_start_idx = []
    buffer_times = []

    for i in range(ITERATIONS):
        torch.cuda.synchronize()
        start_time = time.time()

        # åªç”Ÿæˆlatentï¼ˆä¸è§£ç ï¼‰
        latent = stream.generate_latent()

        torch.cuda.synchronize()
        elapsed = time.time() - start_time

        # ç´¯ç§¯åˆ°buffer
        latent_buffer.append(latent)
        buffer_start_idx.append(i)
        buffer_times.append(elapsed)

        # å½“bufferæ»¡æˆ–æœ€åä¸€æ‰¹æ—¶ï¼Œæ‰¹é‡è§£ç 
        if len(latent_buffer) == VAE_BATCH_SIZE or i == ITERATIONS - 1:
            torch.cuda.synchronize()
            decode_start = time.time()

            # æ‰¹é‡è§£ç 
            latents_batch = torch.cat(latent_buffer, dim=0)
            images_batch = stream.decode_latents(latents_batch)

            torch.cuda.synchronize()
            decode_time = time.time() - decode_start

            # å¹³æ‘Šè§£ç æ—¶é—´åˆ°æ¯å¼ å›¾
            decode_time_per_image = decode_time / len(latent_buffer)

            # ä¿å­˜å›¾åƒå¹¶è®°å½•æ—¶é—´
            for j, (img_idx, gen_time) in enumerate(zip(buffer_start_idx, buffer_times)):
                total_time = gen_time + decode_time_per_image
                results.append(total_time)

                # ä¿å­˜å›¾åƒ
                torchvision.utils.save_image(
                    images_batch[j:j+1],
                    os.path.join(OUTPUT_DIR, f"image_{WIDTH}_{img_idx:06d}.png")
                )

                # æ˜¾ç¤ºè¿›åº¦
                if img_idx % 10 == 0 or img_idx < 10:
                    img_per_sec = 1 / total_time
                    avg_fps = len(results) / sum(results)
                    print(f"å›¾åƒ {img_idx+1:3d}/{ITERATIONS} | FPS: {img_per_sec:6.2f} | å¹³å‡FPS: {avg_fps:6.2f} | ç”Ÿæˆ: {gen_time:.3f}s | è§£ç : {decode_time_per_image:.3f}s")

            # æ¸…ç©ºbuffer
            latent_buffer = []
            buffer_start_idx = []
            buffer_times = []
else:
    print(f"ğŸ“ é€ä¸ªè§£ç æ¨¡å¼ (VAE_BATCH_SIZE=1)")

    # åŸå§‹é€ä¸ªè§£ç æ¨¡å¼ï¼ˆæ‹†åˆ†UNet/VAEè®¡æ—¶ï¼Œä¾¿äºè¯„ä¼°ï¼‰
    for i in range(ITERATIONS):
        torch.cuda.synchronize()
        unet_start = time.time()

        # åªç”Ÿæˆlatent
        latent = stream.generate_latent()

        torch.cuda.synchronize()
        unet_time = time.time() - unet_start

        torch.cuda.synchronize()
        vae_start = time.time()

        # è§£ç 
        sample = stream.decode_latents(latent)

        torch.cuda.synchronize()
        vae_time = time.time() - vae_start

        total_time = unet_time + vae_time
        results.append(total_time)

        # è®¡ç®—æ€§èƒ½
        img_per_sec = 1 / total_time
        avg_fps = len(results) / sum(results)

        # ä¿å­˜å›¾åƒ
        torchvision.utils.save_image(
            sample,
            os.path.join(OUTPUT_DIR, f"image_{WIDTH}_{i:06d}.png")
        )

        # æ˜¾ç¤ºè¿›åº¦
        if i % 10 == 0 or i < 10:
            print(f"å›¾åƒ {i+1:3d}/{ITERATIONS} | FPS: {img_per_sec:6.2f} | å¹³å‡FPS: {avg_fps:6.2f} | ç”Ÿæˆ: {unet_time:.3f}s | è§£ç : {vae_time:.3f}s")

# ================================
# ğŸ“Š æœ€ç»ˆç»Ÿè®¡
# ================================
if results:
    avg_time = sum(results) / len(results)
    total_fps = len(results) / sum(results)
    min_time = min(results)
    max_time = max(results)
    
    print(f"\\n" + "=" * 50)
    print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡")
    print(f"=" * 50)
    print(f"æ€»å›¾åƒæ•°:      {len(results)}")
    print(f"å¹³å‡ç”Ÿæˆæ—¶é—´:  {avg_time:.3f}s")
    print(f"å¹³å‡FPS:       {total_fps:.2f}")
    print(f"æœ€å¿«FPS:       {1/min_time:.2f}")
    print(f"æœ€æ…¢FPS:       {1/max_time:.2f}")
    print(f"æ€»ç”¨æ—¶:        {sum(results):.2f}s")
    print(f"åŠ é€Ÿæ–¹æ³•:      {ACCELERATION}")
    print(f"æµæ°´çº¿æ‰¹é‡:    {'âœ…' if USE_PIPELINE_BATCH else 'âŒ'}")
    print(f"VAE INT8é‡åŒ–:  {'âœ…' if USE_INT8_VAE else 'âŒ'}")
    print(f"VAEæ‰¹é‡è§£ç :   {VAE_BATCH_SIZE}")

    # ğŸ¯ æ€§èƒ½è¯„ä¼°
    print(f"\\nğŸ’¡ æ€§èƒ½è¯„ä¼°:")
    if total_fps >= 12:
        print(f"   ğŸ‰ ä¼˜ç§€ï¼å·²è¾¾åˆ°12 FPSç›®æ ‡ ({total_fps:.1f} FPS)")
    elif total_fps >= 8:
        print(f"   âœ… è‰¯å¥½ï¼æ¥è¿‘ç›®æ ‡ ({total_fps:.1f} FPS)")
    elif total_fps >= 6:
        print(f"   âš¡ ä¸é”™ï¼æœ‰æå‡ç©ºé—´ ({total_fps:.1f} FPS)")
    else:
        print(f"   ğŸ”§ éœ€è¦ä¼˜åŒ– ({total_fps:.1f} FPS)")
    
    # ğŸ¨ è´¨é‡æé†’
    if ACCELERATION == "tensorrt" and USE_PIPELINE_BATCH:
        print(f"   ğŸ¨ TensorRTä¿®å¤ç‰ˆï¼šåº”è¯¥æ¶ˆé™¤äº†å™ªéŸ³é—®é¢˜")
    
    print(f"\\nğŸ“ æ‰€æœ‰å›¾åƒå·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
    
    # ğŸš€ ä½¿ç”¨å»ºè®®
    print(f"\\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    if total_fps < 12:
        if ACCELERATION != "tensorrt":
            print(f"   - å°è¯•è®¾ç½® ACCELERATION = 'tensorrt' è·å¾—æ›´é«˜æ€§èƒ½")
        if not USE_TINY_VAE:
            print(f"   - å°è¯•è®¾ç½® USE_TINY_VAE = True è·å¾—æ›´å¿«é€Ÿåº¦")
        if not USE_PIPELINE_BATCH:
            print(f"   - å°è¯•è®¾ç½® USE_PIPELINE_BATCH = True å¯ç”¨æµæ°´çº¿åŠ é€Ÿ")
    else:
        print(f"   ğŸ‰ é…ç½®å·²ä¼˜åŒ–ï¼äº«å—é«˜æ€§èƒ½ç”Ÿæˆ")

print(f"\\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
print(f"ğŸ” è¯·æ£€æŸ¥å›¾åƒè´¨é‡å’Œè¿ç»­æ€§")

if ACCELERATION == "tensorrt" and USE_PIPELINE_BATCH:
    print(f"\\nğŸ’¡ TensorRTä¿®å¤ç‰ˆä½¿ç”¨æŒ‡å—:")
    print(f"   1. è®¾ç½® ACCELERATION = 'tensorrt'")
    print(f"   2. é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ç¼–è¯‘å¼•æ“")
    print(f"   3. åç»­è¿è¡Œç›´æ¥åŠ è½½å¼•æ“")
    print(f"   4. è‡ªåŠ¨å¤„ç†æ—¶é—´æ­¥å…¼å®¹æ€§")
    print(f"   5. äº«å—æ— å™ªéŸ³çš„TensorRTåŠ é€Ÿï¼")

print(f"\\nğŸ‰ ç”Ÿæˆå®Œæˆï¼")

# æ‰“å°æ˜¾å­˜å³°å€¼ï¼ˆå¤‡ç”¨ç›‘æ§ï¼‰
try:
    peak_mem_mb = torch.cuda.max_memory_reserved() / (1024 * 1024)
    print(f"PEAK_MEM_MB: {peak_mem_mb:.2f}")
except Exception:
    pass
