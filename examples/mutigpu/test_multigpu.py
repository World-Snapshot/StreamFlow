#!/usr/bin/env python3
"""
Test58: ç®€æ´çš„é«˜æ€§èƒ½ç”Ÿæˆå™¨

åŸºäºtest57çš„æ‰€æœ‰åŠŸèƒ½ï¼Œä½†å»æ‰å®éªŒæ€§ä»£ç ï¼Œä¿æŒç®€æ´ï¼š
- ğŸš€ TensorRTä¿®å¤ç‰ˆï¼ˆè‡ªåŠ¨å¤„ç†æ—¶é—´æ­¥å…¼å®¹æ€§ï¼‰
- âš¡ æµæ°´çº¿æ‰¹é‡å»å™ª
- ğŸ”§ ç®€å•é…ç½®ï¼Œç›´æ¥ç”Ÿæˆ
- ğŸ“Š æ¸…æ™°çš„æ€§èƒ½æŠ¥å‘Š
"""

import torch, torchvision
from src.scheduler_perflow import PeRFlowScheduler
import time
import os

from diffusers import AutoencoderTiny, StableDiffusionPipeline
from src.streamflow.pipeline_batch_pipeline import PipelineBatchStreamFlow


def add_tensorrt_timestep_compatibility(stream):
    """
    ğŸš€ å…³é”®ä¿®å¤ï¼šæ·»åŠ TensorRTæ—¶é—´æ­¥å…¼å®¹æ€§
    
    é—®é¢˜ï¼šTensorRTä¸èƒ½å¤„ç†ä¸åŒæ—¶é—´æ­¥ [1000, 750, 500, 250]
    è§£å†³ï¼šåœ¨æ£€æµ‹åˆ°ä¸åŒæ—¶é—´æ­¥æ—¶ï¼Œåˆ†è§£ä¸ºå•ç‹¬è°ƒç”¨
    """
    if not hasattr(stream.unet, 'forward'):
        print("âš ï¸  UNetæ²¡æœ‰forwardæ–¹æ³•ï¼Œè·³è¿‡å…¼å®¹æ€§ä¿®å¤")
        return None
    
    original_forward = stream.unet.forward
    
    def tensorrt_compatible_forward(sample, timestep, encoder_hidden_states, **kwargs):
        """
        TensorRTå…¼å®¹çš„forwardæ–¹æ³•
        è‡ªåŠ¨å¤„ç†ä¸åŒæ—¶é—´æ­¥é—®é¢˜
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸åŒæ—¶é—´æ­¥
        if isinstance(timestep, torch.Tensor) and timestep.dim() > 0 and len(timestep) > 1:
            unique_timesteps = torch.unique(timestep)
            if len(unique_timesteps) > 1:
                # ğŸ¯ æ£€æµ‹åˆ°ä¸åŒæ—¶é—´æ­¥ï¼Œåˆ†è§£å¤„ç†
                batch_size = sample.shape[0]
                results = []
                for i in range(batch_size):
                    single_sample = sample[i:i+1]
                    single_timestep = timestep[i:i+1]
                    single_encoder_states = encoder_hidden_states[i:i+1]
                    
                    single_result = original_forward(
                        single_sample, single_timestep, single_encoder_states, **kwargs
                    )
                    results.append(single_result)
                
                # é‡æ–°ç»„è£…ç»“æœ
                if isinstance(results[0], tuple):
                    assembled = []
                    for i in range(len(results[0])):
                        assembled.append(torch.cat([r[i] for r in results], dim=0))
                    return tuple(assembled)
                else:
                    return torch.cat(results, dim=0)
        
        # ç›¸åŒæ—¶é—´æ­¥æˆ–å•ä¸ªæ—¶é—´æ­¥ï¼Œç›´æ¥è°ƒç”¨TensorRT
        return original_forward(sample, timestep, encoder_hidden_states, **kwargs)
    
    # åº”ç”¨å…¼å®¹æ€§ä¿®å¤
    stream.unet.forward = tensorrt_compatible_forward
    print("âœ… TensorRTæ—¶é—´æ­¥å…¼å®¹æ€§å·²æ·»åŠ ")
    return original_forward


# ================================
# ğŸ”§ é…ç½®åŒºåŸŸ - æ‰€æœ‰è®¾ç½®éƒ½åœ¨è¿™é‡Œ
# ================================

# åŸºç¡€é…ç½®ï¼ˆåŸºäºtest43ï¼‰
USE_TINY_VAE = True              # è®¾ä¸ºTrueå¯è¿›ä¸€æ­¥åŠ é€Ÿï¼Œä½†ä¼šè½»å¾®å½±å“è´¨é‡
ACCELERATION = "xformers"        # "xformers", "tensorrt", "none" - ğŸš€ æµ‹è¯•ä¿®å¤åçš„tensorrt
ITERATIONS = 100                 # ç”Ÿæˆå›¾åƒæ•°é‡

# æµæ°´çº¿é…ç½®
USE_PIPELINE_BATCH = True        # ğŸš€ å…³é”®æ–°å¢ï¼šçœŸæ­£çš„æ‰¹é‡å»å™ªå¼€å…³ True=æµæ°´çº¿æ‰¹é‡å»å™ªï¼ŒFalse=åŸå§‹StreamFlow
CFG_TYPE = "none"               # "none", "full", "self", "initialize" - I usually use none and full
GUIDANCE_SCALE = 7.5            # CFGå¼ºåº¦
NUM_INFERENCE_STEPS = 4         # æ¨ç†æ­¥æ•°

# æç¤ºè¯
PROMPT_BASE = "RAW photo, 8k uhd, dslr, high quality, film grain, highly detailed, masterpiece"
PROMPT_SUBJECT = "A man with brown skin, a beard, and dark eyes"
NEGATIVE_PROMPT = "distorted, blur, smooth, low-quality, warm, haze, over-saturated, high-contrast, out of focus, dark"

# è¾“å‡ºé…ç½®
OUTPUT_DIR = "Multigpu"
SEED = 1024

print("ğŸš€ PeRFlowé«˜æ€§èƒ½multigpuç”Ÿæˆå™¨")
print("=" * 50)
print(f"ğŸ”§ é…ç½®:")
print(f"   VAE: {'TinyVAE' if USE_TINY_VAE else 'åŸå§‹VAE'}")
print(f"   åŠ é€Ÿ: {ACCELERATION}")
print(f"   æµæ°´çº¿æ‰¹é‡: {'âœ…' if USE_PIPELINE_BATCH else 'âŒ'}")
print(f"   ç”Ÿæˆæ•°é‡: {ITERATIONS}")

# ================================
# ğŸ“¦ æ¨¡å‹åŠ è½½
# ================================
print(f"\\nğŸ“¦ åŠ è½½æ¨¡å‹...")

from accelerate import PartialState
distributed_state = PartialState()
with distributed_state.main_process_first():
    pipe = StableDiffusionPipeline.from_pretrained(
        "hansyan/perflow-sd15-dreamshaper", 
        torch_dtype=torch.float16
    )

    pipe.scheduler = PeRFlowScheduler.from_config(
        pipe.scheduler.config, 
        prediction_type="diff_eps", 
        num_time_windows=4
    )


pipe.to(distributed_state.device)

if USE_TINY_VAE:
    with distributed_state.main_process_first():
        vae = AutoencoderTiny.from_pretrained("madebyollin/taesd")
   
    vae.to(
    device=pipe.device, dtype=pipe.dtype)
    pipe.vae = vae
    del vae
# ================================
# ğŸš€ åˆ›å»ºæµæ°´çº¿
# ================================
print("ğŸš€ åˆ›å»ºæµæ°´çº¿...")

stream = PipelineBatchStreamFlow(
    pipe,
    t_index_list=[0, 1, 2, 3],  # PeRFlowçš„4ä¸ªæ—¶é—´æ­¥ [0, 1, 2, 3]ï¼Œä½¿ç”¨49è¿™ç§æ—¶é—´æ­¥ä¼¼ä¹å¯ä»¥æå‡è´¨é‡ï¼š[0, 12, 24, 49]ï¼›ä½†æ˜¯0 1 2 3å¯¹äºcfgä¸ºnoneæ—¶æ•ˆæœéå¸¸å¥½
    torch_dtype=torch.float16,
    frame_buffer_size=1,  # å¸§ç¼“å†²å¤§å°ï¼š1=æ— ç¼“å†²ï¼Œ2-8=å¤šå¸§ç¼“å†²
    cfg_type=CFG_TYPE,  # none, full, self, initialize
    use_pipeline_batch=USE_PIPELINE_BATCH,  # å¯ç”¨æµæ°´çº¿æ‰¹é‡å»å™ª
    vae_decode_method="normalize",  # "normalize" æˆ– "dynamic" - ä¼˜åŒ–çš„è§£ç æ–¹æ³•
    do_add_noise=True,  # æ·»åŠ å™ªå£°ï¼šTrue=æ ‡å‡†æ¨¡å¼ï¼ŒFalse=å¿«é€Ÿæ¨¡å¼
)

# ================================
# âš¡ åŠ é€Ÿè®¾ç½®
# ================================
if ACCELERATION == "xformers":
    pipe.enable_xformers_memory_efficient_attention()
    print("âš¡ xformersåŠ é€Ÿå·²å¯ç”¨")
elif ACCELERATION == "tensorrt":
    print("ğŸš€ å¯ç”¨TensorRTåŠ é€Ÿ...")
    try:
        from src.streamdiffusion.acceleration.tensorrt import accelerate_with_tensorrt
        from src.streamdiffusion.pipeline import StreamDiffusion
        
        # åˆ›å»ºå¼•æ“ç›®å½•
        engine_dir = "tensorrt_engines_test59_fixed"
        os.makedirs(engine_dir, exist_ok=True)
        
        temp_stream = StreamDiffusion(
            pipe, t_index_list=[0, 1, 2, 3], torch_dtype=torch.float16,
            frame_buffer_size=1, cfg_type=CFG_TYPE, use_denoising_batch=True,
            width=512, height=512,
        )
        
        temp_stream.prepare(PROMPT_BASE, NEGATIVE_PROMPT, num_inference_steps=NUM_INFERENCE_STEPS, guidance_scale=GUIDANCE_SCALE)
        
        print("   ç¼–è¯‘TensorRTå¼•æ“...")
        accelerated_stream = accelerate_with_tensorrt(
            temp_stream, engine_dir=engine_dir, max_batch_size=4, min_batch_size=1, use_cuda_graph=False,
        )
        
        stream.unet = accelerated_stream.unet
        
        # ğŸš€ åº”ç”¨æ—¶é—´æ­¥å…¼å®¹æ€§ä¿®å¤
        if USE_PIPELINE_BATCH:
            add_tensorrt_timestep_compatibility(stream)
            print("   æ—¶é—´æ­¥å…¼å®¹æ€§ä¿®å¤å·²åº”ç”¨")
        
        del temp_stream, accelerated_stream
        torch.cuda.empty_cache()
        print("âœ… TensorRTåŠ é€Ÿå·²å¯ç”¨")
        
    except Exception as e:
        print(f"âŒ TensorRTå¤±è´¥ï¼Œå›é€€åˆ°æ— åŠ é€Ÿ: {e}")
        ACCELERATION = "none"

# ================================
# ğŸ”¥ é¢„çƒ­
# ================================
# é¢„çƒ­é˜¶æ®µ
print(f"\\nğŸ”¥ é¢„çƒ­ä¸­...")

with distributed_state.split_between_processes(list(range(10))) as local_idxs:
    generator = torch.Generator(distributed_state.device).manual_seed(SEED)

    prompt_text = f"{PROMPT_BASE}; {PROMPT_SUBJECT}"
    # å‡†å¤‡StreamFlow
    stream.prepare(prompt_text, NEGATIVE_PROMPT, num_inference_steps=NUM_INFERENCE_STEPS, guidance_scale=GUIDANCE_SCALE)

    # é¢„çƒ­ç”Ÿæˆ
    for i in range(10):
        _ = stream.txt2img()
        if i == 0:
            print("âœ… é¢„çƒ­å®Œæˆ")

# ================================
# ğŸ¨ å›¾åƒç”Ÿæˆ
# ================================
print(f"\\nğŸ¨ å¼€å§‹ç”Ÿæˆ {ITERATIONS} å¼ å›¾åƒ...")
print(f"ğŸ“ æç¤ºè¯: {prompt_text}")

os.makedirs(OUTPUT_DIR, exist_ok=True)
results = []
torch.cuda.synchronize()
start_time = time.time()
with distributed_state.split_between_processes(list(range(ITERATIONS))) as local_idxs:
    
    print(f"{len(local_idxs)} image per gpu")
    print(f'using {ITERATIONS//len(local_idxs)} GPUs')

    # ç”Ÿæˆå›¾åƒ
    for idx in local_idxs:
        sample = stream.txt2img()
        

        torchvision.utils.save_image(
            sample,
            os.path.join(OUTPUT_DIR, f"image_{idx:06d}.png")
        )
torch.cuda.synchronize()
elapsed = time.time() - start_time    

# ================================
# ğŸ“Š æœ€ç»ˆç»Ÿè®¡
# ================================

print(f"\\n" + "=" * 50)
print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡")
print(f"=" * 50)
print(f"æ€»å›¾åƒæ•°:      {ITERATIONS}")
print(f"å¹³å‡ç”Ÿæˆæ—¶é—´:  {elapsed/ITERATIONS:.3f}s")
print(f"æ€»ç”¨æ—¶:        {elapsed:.2f}s")
print(f"FPS:        {ITERATIONS/elapsed:.2f}")
print(f"åŠ é€Ÿæ–¹æ³•:      {ACCELERATION}")
print(f"æµæ°´çº¿æ‰¹é‡:    {'âœ…' if USE_PIPELINE_BATCH else 'âŒ'}")
    

print(f"\\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
print(f"ğŸ” è¯·æ£€æŸ¥å›¾åƒè´¨é‡å’Œè¿ç»­æ€§")

if ACCELERATION == "tensorrt" and USE_PIPELINE_BATCH:
    print(f"\\nğŸ’¡ TensorRTä¿®å¤ç‰ˆä½¿ç”¨æŒ‡å—:")
    print(f"   1. è®¾ç½® ACCELERATION = 'tensorrt'")
    print(f"   2. é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ç¼–è¯‘å¼•æ“")
    print(f"   3. åç»­è¿è¡Œç›´æ¥åŠ è½½å¼•æ“")
    print(f"   4. è‡ªåŠ¨å¤„ç†æ—¶é—´æ­¥å…¼å®¹æ€§")
    print(f"   5. äº«å—æ— å™ªéŸ³çš„TensorRTåŠ é€Ÿï¼")

print(f"\\nğŸ‰ ç”Ÿæˆå®Œæˆï¼")